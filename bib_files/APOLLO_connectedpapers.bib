@article{ec6a8bc007ba9573809ff9a972f6ed300dfd89ce,
title = {APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning},
year = {2025},
url = {https://www.semanticscholar.org/paper/ec6a8bc007ba9573809ff9a972f6ed300dfd89ce},
abstract = {Formal reasoning and automated theorem proving constitute a challenging subfield of machine learning, in which machines are tasked with proving mathematical theorems using formal languages like Lean. A formal verification system can check whether a formal proof is correct or not almost instantaneously, but generating a completely correct formal proof with LLMs remains a formidable task. The usual approach in the literature is to prompt the LLM many times (up to several thousands) until one of the generated proofs passes the verification system. In this work, we present APOLLO (Automated PrOof repair via LLM and Lean cOllaboration), a modular, modelagnostic pipeline that combines the strengths of the Lean compiler with an LLM's reasoning abilities to achieve better proofgeneration results at a low sampling budget. Apollo directs a fully automated process in which the LLM generates proofs for theorems, a set of agents analyze the proofs, fix the syntax errors, identify the mistakes in the proofs using Lean, isolate failing sublemmas, utilize automated solvers, and invoke an LLM on each remaining goal with a low budget. The repaired subproofs are recombined and reverified, iterating up to a usercontrolled maximum number of attempts. On the miniF2F benchmark, we establish a new stateoftheart accuracy of 84.9% among sub 8Bparameter models while keeping the sampling budget below one hundred. Moreover, Apollo raises the stateoftheart accuracy for GoedelProverSFT to 65.6% while cutting sample complexity from 25,600 to a few hundred. Generalpurpose models (o3mini, o4mini) jump from 3-7% to over 40% accuracy. Our results demonstrate that targeted, compilerguided repair of LLM outputs yields dramatic gains in both efficiency and correctness, suggesting a general paradigm for scalable automated theorem proving. The codebase is available at https://github.com/aziksh-ospanov/APOLLO},
author = {Azim Ospanov and Farzan Farnia and Roozbeh Yousefzadeh},
journal = {ArXiv},
volume = {abs/2505.05758},
pages = {null},
doi = {10.48550/arXiv.2505.05758},
arxivid = {2505.05758},
}

@article{f9e701ac5d025581f519eae1216e26475e56462b,
title = {STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/f9e701ac5d025581f519eae1216e26475e56462b},
abstract = {A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens generated during the training in Lean, STP proves 28.5% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (65.0%, pass@3200), Proofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release our code, model, and dataset in this URL: https://github.com/kfdong/STP.},
author = {Kefan Dong and Tengyu Ma},
journal = {ArXiv},
volume = {abs/2502.00212},
pages = {null},
doi = {10.48550/arXiv.2502.00212},
arxivid = {2502.00212},
}

@article{f13fd43670c1fbec996eef685cf3030784779f16,
title = {Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation},
year = {2024},
url = {https://www.semanticscholar.org/paper/f13fd43670c1fbec996eef685cf3030784779f16},
abstract = {Formal proofs are challenging to write even for experienced experts. Recent progress in Neural Theorem Proving (NTP) shows promise in expediting this process. However, the formal corpora available on the Internet are limited compared to the general text, posing a significant data scarcity challenge for NTP. To address this issue, this work proposes Alchemy, a general framework for data synthesis that constructs formal theorems through symbolic mutation. Specifically, for each candidate theorem in Mathlib, we identify all invocable theorems that can be used to rewrite or apply to it. Subsequently, we mutate the candidate theorem by replacing the corresponding term in the statement with its equivalent form or antecedent. As a result, our method increases the number of theorems in Mathlib by an order of magnitude, from 110k to 6M. Furthermore, we perform continual pretraining and supervised finetuning on this augmented corpus for large language models. Experimental results demonstrate the effectiveness of our approach, achieving a 4.70% absolute performance improvement on Leandojo benchmark. Additionally, our approach achieves a 2.47% absolute performance gain on the out-of-distribution miniF2F benchmark based on the synthetic data.To provide further insights, we conduct a comprehensive analysis of synthetic data composition and the training paradigm, offering valuable guidance for developing a strong theorem prover.},
author = {Shaonan Wu and Shuai Lu and Yeyun Gong and Nan Duan and Ping Wei},
journal = {ArXiv},
volume = {abs/2410.15748},
pages = {null},
doi = {10.48550/arXiv.2410.15748},
arxivid = {2410.15748},
}

@article{1a30dfd53be562d10a7cc33bf475c1c6569f659e,
title = {FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/1a30dfd53be562d10a7cc33bf475c1c6569f659e},
abstract = {Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.},
author = {Zhouliang Yu and Ruotian Peng and Keyi Ding and Yizhe Li and Zhongyuan Peng and Minghao Liu and Yifan Zhang and Zheng Yuan and Huajian Xin and Wenhao Huang and Yandong Wen and Ge Zhang and Weiyang Liu},
journal = {ArXiv},
volume = {abs/2505.02735},
pages = {null},
doi = {10.48550/arXiv.2505.02735},
arxivid = {2505.02735},
}

@article{4c6f256693479c20141abf3a0f7865773da69571,
title = {Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions},
year = {2025},
url = {https://www.semanticscholar.org/paper/4c6f256693479c20141abf3a0f7865773da69571},
abstract = {Mathematical reasoning lies at the heart of artificial intelligence, underpinning applications in education, program verification, and research-level mathematical discovery. Mathematical competitions, in particular, present two challenging problem types: theorem proving, which requires rigorous proofs of stated conclusions, and answer construction, which involves hypothesizing and formally verifying mathematical objects. Large Language Models (LLMs) effectively generate creative candidate answers but struggle with formal verification, while symbolic provers ensure rigor but cannot efficiently handle creative conjecture generation. We introduce the Enumerate-Conjecture-Prove (ECP) framework, a modular neuro-symbolic method integrating LLM-based enumeration and pattern-driven conjecturing with formal theorem proving. We present ConstructiveBench, a dataset of 3,431 answer-construction problems in various math competitions with verified Lean formalizations. On the ConstructiveBench dataset, ECP improves the accuracy of answer construction from a Chain-of-Thought (CoT) baseline of 14.54% to 45.06% with the gpt-4.1-mini model. Moreover, combined with ECP's constructed answers, the state-of-the-art DeepSeek-Prover-V2-7B model generates correct proofs for 858 of the 3,431 constructive problems in Lean, achieving 25.01% accuracy compared to 9.86% for symbolic-only baselines. Our code and dataset are publicly available at https://github.com/JackSun200312/ECP.},
author = {Jialiang Sun and Yuzhi Tang and Ao Li and Chris J. Maddison and Kuldeep S. Meel},
journal = {ArXiv},
volume = {abs/2505.18492},
pages = {null},
doi = {10.48550/arXiv.2505.18492},
arxivid = {2505.18492},
}

@article{d6fa3cfde46c45d746853b39d7cc420ec96d8f97,
title = {Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning},
year = {2025},
url = {https://www.semanticscholar.org/paper/d6fa3cfde46c45d746853b39d7cc420ec96d8f97},
abstract = {We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover},
author = {Haiming Wang and Mert Unsal and Xiaohan Lin and Mantas Baksys and Junqi Liu and Marco Dos Santos and Flood Sung and Marina Vinyes and Zhenzhe Ying and Zekai Zhu and Jianqiao Lu and Hugues de Saxc'e and Bolton Bailey and Chendong Song and Chenjun Xiao and Dehao Zhang and Ebony Zhang and Frederick Pu and Han Zhu and Jiawei Liu and Jonas Bayer and Julien Michel and Long Yu and L. Dreyfus-Schmidt and Lewis Tunstall and Luigi Pagani and Moreira Machado and Pauline Bourigault and Ran Wang and Stanislas Polu and Thibaut Barroyer and Wen-Ding Li and Yazhe Niu and Y. Fleureau and Yang Hu and Zhouliang Yu and Zihan Wang and Zhilin Yang and Zhengying Liu and Jia Li},
journal = {ArXiv},
volume = {abs/2504.11354},
pages = {null},
doi = {10.48550/arXiv.2504.11354},
arxivid = {2504.11354},
}

@article{473ec8d855adc1dbbfbb562d0a7c425a45b28e06,
title = {3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes},
year = {2024},
url = {https://www.semanticscholar.org/paper/473ec8d855adc1dbbfbb562d0a7c425a45b28e06},
abstract = {A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D-Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F-valid and miniF2F-test benchmarks by augmenting the ReProver LLM. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity.},
author = {Sean Lamont and Christian Walder and Amir Dezfouli and Paul Montague and Michael Norrish},
journal = {ArXiv},
volume = {abs/2410.11133},
pages = {null},
doi = {10.48550/arXiv.2410.11133},
arxivid = {2410.11133},
}

@article{b308f9f7e60e04c33319f7459993bb33d5aff763,
title = {Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/b308f9f7e60e04c33319f7459993bb33d5aff763},
abstract = {Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .},
author = {Zhenwen Liang and Linfeng Song and Yang Li and Tao Yang and Feng Zhang and Haitao Mi and Dong Yu},
journal = {ArXiv},
volume = {abs/2507.06804},
pages = {null},
doi = {10.48550/arXiv.2507.06804},
arxivid = {2507.06804},
}

@article{11ab53416b97f94df7d38df80b01d11a228bc6c3,
title = {Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification},
year = {2025},
url = {https://www.semanticscholar.org/paper/11ab53416b97f94df7d38df80b01d11a228bc6c3},
abstract = {Formally verifying properties of software code has been a highly desirable task, especially with the emergence of LLM-generated code. In the same vein, they provide an interesting avenue for the exploration of formal verification and mechanistic interpretability. Since the introduction of code-specific models, despite their successes in generating code in Lean4 and Isabelle, the task of generalized theorem proving still remains far from being fully solved and will be a benchmark for reasoning capability in LLMs. In this work, we introduce a framework that generates whole proofs in a formal language to be used within systems that utilize the power of built-in tactics and off-the-shelf automated theorem provers. Our framework includes 3 components: generating natural language statements of the code to be verified, an LLM that generates formal proofs for the given statement, and a module employing heuristics for building the final proof. To train the LLM, we employ a 2-stage fine-tuning process, where we first use SFT-based training to enable the model to generate syntactically correct Isabelle code and then RL-based training that encourages the model to generate proofs verified by a theorem prover. We validate our framework using the miniF2F-test benchmark and the Isabelle proof assistant and design a use case to verify the correctness of the AWS S3 bucket access policy code. We also curate a dataset based on the FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.},
author = {Balaji Rao and William Eiers and Carlo Lipizzi},
journal = {ArXiv},
volume = {abs/2504.17017},
pages = {null},
doi = {10.48550/arXiv.2504.17017},
arxivid = {2504.17017},
}

@article{faed60347348c362e11c3158a43c3bec851afec0,
title = {Prover Agent: An Agent-based Framework for Formal Mathematical Proofs},
year = {2025},
url = {https://www.semanticscholar.org/paper/faed60347348c362e11c3158a43c3bec851afec0},
abstract = {We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 86.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present case studies illustrating how these generated lemmas contribute to solving challenging problems.},
author = {Kaito Baba and Chaoran Liu and Shuhei Kurita and Akiyoshi Sannai},
journal = {ArXiv},
volume = {abs/2506.19923},
pages = {null},
doi = {10.48550/arXiv.2506.19923},
arxivid = {2506.19923},
}

@article{72922530fcfa7c1218b2e5a17851f7a36ee05053,
title = {BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/72922530fcfa7c1218b2e5a17851f7a36ee05053},
abstract = {Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of $72.95\%$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. To facilitate further research and development in this area, we have open-sourced our model at https://huggingface.co/bytedance-research/BFS-Prover.},
author = {Ran Xin and Chenguang Xi and Jie Yang and Feng Chen and Hang Wu and Xia Xiao and Yifan Sun and Shen Zheng and Kai Shen},
journal = {ArXiv},
volume = {abs/2502.03438},
pages = {32588-32599},
doi = {10.48550/arXiv.2502.03438},
arxivid = {2502.03438},
}

@article{2587ee3880c15a247f8f5fca40ffdef088102100,
title = {Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening},
year = {2025},
url = {https://www.semanticscholar.org/paper/2587ee3880c15a247f8f5fca40ffdef088102100},
abstract = {Reinforcement learning is emerging as a primary driver for improving language model reasoning capabilities. A fundamental question is whether current reinforcement learning algorithms -- such as Group Relative Policy Optimization (GRPO), the de facto standard algorithm used to improve language model reasoning -- merely sharpen the base model's distribution around problems it can already solve. We investigate this question in the context of formal theorem proving, which has access to a perfect verifier. We identify a degenerate rank bias in GRPO in which highly probable trajectories are reinforced and rare ones are neglected. This results in distribution sharpening: the model can solve some problems with fewer samples, but underperforms simply sampling more solutions from the original model. To overcome GRPO's rank bias we introduce unlikeliness reward, a simple method for explicitly up-weighting rare but correct solutions. We show that unlikeliness reward mitigates rank bias and improves pass@$N$ across a large range of $N$ in both synthetic and real theorem proving settings. We also uncover an unexpected link between rank bias and a seemingly mundane hyperparameter -- the number of updates per batch -- that leads to a second, complementary mitigation. We combine our insights into a revised GRPO training recipe for formal theorem proving, yielding an open pipeline that achieves competitive performance to DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We release our implementation at https://github.com/AndreHe02/rewarding-unlikely-release},
author = {Andre He and Daniel Fried and S. Welleck},
journal = {ArXiv},
volume = {abs/2506.02355},
pages = {null},
doi = {10.48550/arXiv.2506.02355},
arxivid = {2506.02355},
}

@article{98d04a3b33e919769169b68875b2c46b5055e5a8,
title = {Clarifying Before Reasoning: A Coq Prover with Structural Context},
year = {2025},
url = {https://www.semanticscholar.org/paper/98d04a3b33e919769169b68875b2c46b5055e5a8},
abstract = {In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\times$ improvement in clarity score (44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model \texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.},
author = {Yanzhen Lu and Hanbin Yang and Xiaodie Wang and Ge Zhang and Biao Li and Chenxu Fu and Chao Li and Yang Yuan and A. C. Yao},
journal = {ArXiv},
volume = {abs/2507.02541},
pages = {null},
doi = {10.48550/arXiv.2507.02541},
arxivid = {2507.02541},
}

@article{1ff3d0a6a97aff7717409307bc730b5bc88c8f65,
title = {Leanabell-Prover: Posttraining Scaling in Formal Reasoning},
year = {2025},
url = {https://www.semanticscholar.org/paper/1ff3d0a6a97aff7717409307bc730b5bc88c8f65},
abstract = {Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.},
author = {Jingyuan Zhang and Qi Wang and Xingguang Ji and Yahui Liu and Yang Yue and Fuzheng Zhang and Di Zhang and Guorui Zhou and Kun Gai},
journal = {ArXiv},
volume = {abs/2504.06122},
pages = {null},
doi = {10.48550/arXiv.2504.06122},
arxivid = {2504.06122},
}

@article{c98038be92c3d8bf0e2150f702830373ba708586,
title = {Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/c98038be92c3d8bf0e2150f702830373ba708586},
abstract = {LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.},
author = {Luoxin Chen and Jinming Gu and Liankai Huang and Wenhao Huang and Zhicheng Jiang and Allan Jie and Xiaoran Jin and Xing Jin and Chenggang Li and Kaijing Ma and Cheng Ren and Jiawei Shen and Wenlei Shi and Tong Sun and He Sun and Jiahui Wang and Siran Wang and Zhihong Wang and Chenrui Wei and Shufa Wei and Yong-Xu Wu and Yuchen Wu and Yihang Xia and Hua Xin and Fan Yang and Huaiyuan Ying and Hongyi Yuan and Zheng Yuan and Tianyang Zhan and Chi Zhang and Yue Zhang and Ge-Hang Zhang and Tianyun Zhao and Jianqiu Zhao and Yichi Zhou and Thomas (Hanwen) Zhu},
arxivid = {2507.23726},
}

@article{217b435312f92b627ec619133464d20634039427,
title = {HUNYUANPROVER: A Scalable Data Synthesis Framework and Guided Tree Search for Automated Theorem Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/217b435312f92b627ec619133464d20634039427},
abstract = {We introduce HunyuanProver, an language model finetuned from the Hunyuan 7B for interactive automatic theorem proving with LEAN4. To alleviate the data sparsity issue, we design a scalable framework to iterative synthesize data with low cost. Besides, guided tree search algorithms are designed to enable effective ``system 2 thinking`` of the prover. HunyuanProver achieves state-of-the-art (SOTA) performances on major benchmarks. Specifically, it achieves a pass of 68.4% on the miniF2F-test compared to 65.9%, the current SOTA results. It proves 4 IMO statements (imo_1960_p2, imo_1962_p2}, imo_1964_p2 and imo_1983_p6) in miniF2F-test. To benefit the community, we will open-source a dataset of 30k synthesized instances, where each instance contains the original question in natural language, the converted statement by autoformalization, and the proof by HunyuanProver.},
author = {Yang Li and Dong Du and Linfeng Song and Chen Li and Weikang Wang and Tao Yang and Haitao Mi},
journal = {ArXiv},
volume = {abs/2412.20735},
pages = {null},
doi = {10.48550/arXiv.2412.20735},
arxivid = {2412.20735},
}

@article{0de4d81b8318633d065694d1816d8cf5a3f7ba95,
title = {Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis},
year = {2025},
url = {https://www.semanticscholar.org/paper/0de4d81b8318633d065694d1816d8cf5a3f7ba95},
abstract = {The synergy between deep learning models and traditional automation tools, such as built-in tactics of the proof assistant and off-the-shelf automated theorem provers, plays a crucial role in developing robust and efficient neural theorem provers(NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when explicitly invoked by the model or at a single granularity level, failing to fully exploit their power. To solve this issue, we propose ProofAug, a procedure that equips LLMs with automation methods at various granularities through fine-grained structure analysis of model-generated proof proposals. ProofAug also serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version) with 2100 queries to the model per problem (In contrast, the previous SOTA in Isabelle, Subgoal-XL, only achieves 56.1% using 16384 queries per problem). We also implement a Lean 4 version of ProofAug that can improve the pass@1 performance of Kimina-Prover-Preview-Distill-1.5B from 44.3% to 50.4% on miniF2F-test. Our code is available at https://github.com/haoxiongliu/ProofAug.},
author = {Haoxiong Liu and Jiacheng Sun and Zhenguo Li and Andrew C Yao},
journal = {ArXiv},
volume = {abs/2501.18310},
pages = {null},
doi = {10.48550/arXiv.2501.18310},
arxivid = {2501.18310},
}

@article{2ea01c09635cdb383742e547b00e9e598c344cd8,
title = {Hierarchical Attention Generates Better Proofs},
year = {2025},
url = {https://www.semanticscholar.org/paper/2ea01c09635cdb383742e547b00e9e598c344cd8},
abstract = {Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\% on miniF2F and 1.69\% on ProofNet while reducing proof complexity by 23.81\% and 16.50\% respectively. The code is available at https://github.com/Car-pe/HAGBP.},
author = {Jianlong Chen and Chao Li and Yang Yuan and A. C. Yao},
doi = {10.48550/arXiv.2504.19188},
arxivid = {2504.19188},
}

@article{a7ab47f93748a0d839bb9bd5be6376384f8ff34b,
title = {StepFun-Prover Preview: Let's Think and Verify Step by Step},
year = {2025},
url = {https://www.semanticscholar.org/paper/a7ab47f93748a0d839bb9bd5be6376384f8ff34b},
abstract = {We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of $70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.},
author = {Shijie Shang and Ruosi Wan and Yue Peng and Yutong Wu and Xiong-hui Chen and Jie Yang and Xiangyu Zhang},
journal = {ArXiv},
volume = {abs/2507.20199},
pages = {null},
doi = {10.48550/arXiv.2507.20199},
arxivid = {2507.20199},
}

@article{f96b1186617c2135fa5b6768c896d9e565535fdb,
title = {LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4},
year = {2025},
url = {https://www.semanticscholar.org/paper/f96b1186617c2135fa5b6768c896d9e565535fdb},
abstract = {Automated theorem proving (ATP) has been a classical problem in artificial intelligence since its inception, yet it remains challenging due to its vast state and action space. Large language models (LLMs) have recently emerged as a promising heuristic for ATP, but they lack correctness guarantees and thus require interaction with a proof verifier. Such interactions typically follow one of two approaches: black-box interaction, which does not utilize intermediate proof states, or white-box approaches, which allow for incremental proof construction and examination of intermediate states. While black-box approaches have directly benefited from recent LLM advances, white-box methods have comparatively lagged behind. In this paper, we address this gap by introducing LeanTree, which consists of (i) a tool built in the Lean 4 language that factorizes complex proof states into simpler, independent branches, and (ii) a dataset of these factorized intermediate states. Our white-box tooling offers several advantages over black-box approaches: it simplifies evaluation, reduces necessary context, generates richer training data, enables parallel search across multiple states, supports efficient reuse of states, and provides feedback in case of errors. Our preliminary results hint that white-box approaches outperform black-box alternatives in some settings.},
author = {Matej Kripner and Michal Sustr and Milan Straka},
journal = {ArXiv},
volume = {abs/2507.14722},
pages = {null},
doi = {10.48550/arXiv.2507.14722},
arxivid = {2507.14722},
}

@article{4b94c0cdc52a0cb81d4400109a929d5cf97a3c7a,
title = {REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning},
year = {2025},
url = {https://www.semanticscholar.org/paper/4b94c0cdc52a0cb81d4400109a929d5cf97a3c7a},
abstract = {Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).},
author = {Ziju Shen and N. Huang and Fanyi Yang and Yutong Wang and Guoxiong Gao and Tianyi Xu and Jiedong Jiang and Wanyi He and Pu Yang and Mengzhou Sun and Haocheng Ju and Peihao Wu and Bryan Dai and Bin Dong},
journal = {ArXiv},
volume = {abs/2505.20613},
pages = {null},
doi = {10.48550/arXiv.2505.20613},
arxivid = {2505.20613},
}

@article{27926dae8c5bcf4d67c0bafd7ddd4136fc46b295,
title = {MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation},
year = {2025},
url = {https://www.semanticscholar.org/paper/27926dae8c5bcf4d67c0bafd7ddd4136fc46b295},
abstract = {Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.},
author = {Zhenwen Liang and Linfeng Song and Yang Li and Tao Yang and Feng Zhang and Haitao Mi and Dong Yu},
journal = {ArXiv},
volume = {abs/2505.10962},
pages = {null},
doi = {10.48550/arXiv.2505.10962},
arxivid = {2505.10962},
}

@article{819c9a89d99d9099b3237b27f8cebc7808be6b87,
title = {Mathesis: Towards Formal Theorem Proving from Natural Languages},
year = {2025},
url = {https://www.semanticscholar.org/paper/819c9a89d99d9099b3237b27f8cebc7808be6b87},
abstract = {Recent advances in large language models show strong promise for formal reasoning. However, most LLM-based theorem provers have long been constrained by the need for expert-written formal statements as inputs, limiting their applicability to real-world problems expressed in natural language. We tackle this gap with Mathesis, the first end-to-end theorem proving pipeline processing informal problem statements. It contributes Mathesis-Autoformalizer, the first autoformalizer using reinforcement learning to enhance the formalization ability of natural language problems, aided by our novel LeanScorer framework for nuanced formalization quality assessment. It also proposes a Mathesis-Prover, which generates formal proofs from the formalized statements. To evaluate the real-world applicability of end-to-end formal theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex problems from China's national college entrance exam. Our approach is carefully designed, with a thorough study of each component. Experiments demonstrate Mathesis's effectiveness, with the autoformalizer outperforming the best baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.},
author = {Xuejun Yu and Jianyuan Zhong and Zijin Feng and Pengyi Zhai and Roozbeh Yousefzadeh and Wei Chong Ng and Haoxiong Liu and Ziyi Shou and Jing Xiong and Yudong Zhou and Claudia Beth Ong and Austen Jeremy Sugiarto and Yaoxi Zhang and Wai Ming Tai and Huan Cao and Dong-Lin Lu and Jiacheng Sun and Qiang Xu and Shen Xin and Zhenguo Li},
journal = {ArXiv},
volume = {abs/2506.07047},
pages = {null},
doi = {10.48550/arXiv.2506.07047},
arxivid = {2506.07047},
}

@article{cd716e9a5edf6394f12aad016da864646af6d676,
title = {Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically},
year = {2024},
url = {https://www.semanticscholar.org/paper/cd716e9a5edf6394f12aad016da864646af6d676},
abstract = {Mathematical theorem proving is an important testbed for large language models' deep and abstract reasoning capability. This paper focuses on improving LLMs' ability to write proofs in formal languages that permit automated proof verification/evaluation. Most previous results provide human-written lemmas to the theorem prover, which is an arguably oversimplified setting that does not sufficiently test the provers' planning and decomposition capabilities. Instead, we work in a more natural setup where the lemmas that are directly relevant to the theorem are not given to the theorem prover at test time. We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas. Our reward mechanism is inspired by how mathematicians train themselves: even if a theorem is too challenging to be proved by the current model, a positive reward is still given to the model for any correct and novel lemmas that are proposed and proved in this process. During training, our model proposes and proves lemmas that are not in the training dataset. In fact, these newly-proposed correct lemmas consist of 37.7% of the training replay buffer when we train on the dataset extracted from Archive of Formal Proofs (AFP). The model trained by our RL algorithm outperforms that trained by supervised finetuning, improving the pass rate from 40.8% to 45.5% on AFP test set, and from 36.5% to 39.5% on an out-of-distribution test set.},
author = {Kefan Dong and Arvind V. Mahankali and Tengyu Ma},
journal = {ArXiv},
volume = {abs/2411.01829},
pages = {null},
doi = {10.48550/arXiv.2411.01829},
arxivid = {2411.01829},
}

@article{c116dd5aac94385dcd8626e4f4991d99e9ce5451,
title = {Solving Formal Math Problems by Decomposition and Iterative Reflection},
year = {2025},
url = {https://www.semanticscholar.org/paper/c116dd5aac94385dcd8626e4f4991d99e9ce5451},
abstract = {General-purpose Large Language Models (LLMs) have achieved remarkable success in intelligence, performing comparably to human experts on complex reasoning tasks such as coding and mathematical reasoning. However, generating formal proofs in specialized languages like Lean 4 remains a significant challenge for these models, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning on dedicated formal corpora, incurring high costs for data collection and training. In this work, we introduce \textbf{Delta Prover}, an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages the reflection and reasoning capabilities of general-purpose LLMs to interactively construct formal proofs in Lean 4, circumventing the need for model specialization. At its core, the agent integrates two novel, interdependent components: an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test benchmark, surpassing all existing approaches, including those requiring model specialization.} Furthermore, Delta Prover exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies. Crucially, our findings demonstrate that general-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities. This presents a computationally efficient alternative to specialized models for robust automated reasoning in formal environments.},
author = {Yichi Zhou and Jianqiu Zhao and Yongxin Zhang and Bohan Wang and Siran Wang and Luoxin Chen and Jiahui Wang and Haowei Chen and Allan Jie and Xinbo Zhang and Haocheng Wang and L. Trung and Rong Ye and Phan Nhat Hoang and Huishuai Zhang and Peng Sun and Hang Li},
journal = {ArXiv},
volume = {abs/2507.15225},
pages = {null},
doi = {10.48550/arXiv.2507.15225},
arxivid = {2507.15225},
}

@article{7bbdd993319f6b0f188fbb38f6263022d54a31d9,
title = {Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/7bbdd993319f6b0f188fbb38f6263022d54a31d9},
abstract = {Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.},
author = {Matthieu Zimmer and Xiaotong Ji and Rasul Tutunov and Anthony Bordg and Jun Wang and Haitham Bou-Ammar},
journal = {ArXiv},
volume = {abs/2507.02726},
pages = {null},
doi = {10.48550/arXiv.2507.02726},
arxivid = {2507.02726},
}

@article{e1a642026fb46a8b8a868862bcf0728e8d215d7e,
title = {DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search},
year = {2024},
url = {https://www.semanticscholar.org/paper/e1a642026fb46a8b8a868862bcf0728e8d215d7e},
abstract = {We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark ($63.5\%$) and the undergraduate level ProofNet benchmark ($25.3\%$).},
author = {Huajian Xin and Z. Ren and Jun-Mei Song and Zhihong Shao and Wanjia Zhao and Haocheng Wang and Bo Liu (Benjamin Liu) and Liyue Zhang and Xuan Lu and Qiushi Du and W. Gao and Qihao Zhu and Dejian Yang and Zhibin Gou and Z. F. Wu and Fuli Luo and C. Ruan},
journal = {ArXiv},
volume = {abs/2408.08152},
pages = {null},
doi = {10.48550/arXiv.2408.08152},
arxivid = {2408.08152},
}

@article{616739238dbd02e4e748025a2d52044ad7865d36,
title = {A Lean Dataset for International Math Olympiad: Small Steps towards Writing Math Proofs for Hard Problems},
year = {2024},
url = {https://www.semanticscholar.org/paper/616739238dbd02e4e748025a2d52044ad7865d36},
abstract = {Using AI to write formal proofs for mathematical problems is a challenging task that has seen some advancements in recent years. Automated systems such as Lean can verify the correctness of proofs written in formal language, yet writing the proofs in formal language can be challenging for humans and machines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal proofs are available only for 6 of these problems (3 of which are only written by mathematicians). The model with best accuracy can only prove 2 of these 20 IMO problems, from 1950s and 60s, while its training set is a secret. In this work, we write complete, original formal proofs for the remaining IMO problems in Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands the availability of proof currently in the public domain by creating 5,880 lines of Lean proof. The goal of the paper is to pave the way for developing AI models that can automatically write the formal proofs for all the IMO problems in miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we devise a method to decompose the proofs of these problems into their building blocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean code. These lemmas are not trivial, yet they are approachable, providing the opportunity to evaluate and diagnose the failures and successes of AI models. We evaluate the ability of the SOTA LLMs on our dataset and analyze their success and failure modes from different perspectives. Our dataset and code is available at: https://github.com/roozbeh-yz/IMO-Steps.},
author = {Roozbeh Yousefzadeh and Xuenan Cao},
journal = {Trans. Mach. Learn. Res.},
volume = {2025},
pages = {null},
doi = {10.48550/arXiv.2411.18872},
arxivid = {2411.18872},
}

@article{1517dd0533b24ae89c8600ecb1c8004a8f4dfd27,
title = {Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning},
year = {2025},
url = {https://www.semanticscholar.org/paper/1517dd0533b24ae89c8600ecb1c8004a8f4dfd27},
abstract = {We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that can produce formal theorem proofs in Lean 4, with verifier-integrated Long Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we continual to choose to posttrain existing strong prover models for further performance improvement. In our V2 version, we mainly upgrade the Reinforcement Learning (RL) with feedback provided by the Lean 4 verifier. Crucially, verifier feedback, such as indicating success or detailing specific errors, allows the LLM to become ``self-aware''of the correctness of its own reasoning process and learn to reflexively correct errors. Leanabell-Prover-V2 directly optimizes LLM reasoning trajectories with multi-turn verifier interactions, together with feedback token masking for stable RL training and a simple reward strategy. Experiments show that Leanabell-Prover-V2 improves performance by 3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data and models are available at: https://github.com/Leanabell-LM/Leanabell-Prover-V2.},
author = {Xingguang Ji and Yahui Liu and Qi Wang and Jingyuan Zhang and Yang Yue and Rui Shi and Chenxi Sun and Fuzheng Zhang and Guorui Zhou and Kun Gai},
journal = {ArXiv},
volume = {abs/2507.08649},
pages = {null},
doi = {10.48550/arXiv.2507.08649},
arxivid = {2507.08649},
}

@article{8d6839662f4c0e564be11860f24328d35ad90512,
title = {MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/8d6839662f4c0e564be11860f24328d35ad90512},
abstract = {Solving mathematical problems using computer-verifiable languages like Lean has significantly impacted the mathematical and computer science communities. State-of-the-art methods utilize a single Large Language Model (LLM) to generate complete proof or perform tree search, but they fail to balance these tasks. We propose **MA-LoT**: *Model-CollAboration Lean-based Long Chain-of-Thought*, a comprehensive framework for Lean4 theorem proving to solve this issue. It separates the cognition tasks of general NL for whole-proof generation and error analysis for proof correction using the model-collaboration method. We achieve this by structured interaction of the LLM and Lean4 verifier in Long CoT. To implement the framework, we propose the novel *LoT-Transfer Learning* training-inference pipeline, which enables the Long CoT thinking capability to LLMs without special data annotation. Extensive experiment shows that our framework achieves a **61.07%** accuracy rate on the Lean4 version of the MiniF2F-Test dataset, largely outperforming DeepSeek-V3 (33.61%), single-model tree search (InternLM-Step-Prover, 50.70%), and whole-proof generation (Godel-Prover, 55.33%) baselines. Furthermore, our findings highlight the potential of combining Long CoT with formal verification for a more insightful generation in a broader perspective.},
author = {Ruida Wang and Rui Pan and Yuxin Li and Jipeng Zhang and Yizhen Jia and Shizhe Diao and Renjie Pi and Junjie Hu and Tong Zhang},
arxivid = {2503.03205},
}

@article{036650e109d578a39aba2074a5bf9a0bc6453ea5,
title = {Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities},
year = {2025},
url = {https://www.semanticscholar.org/paper/036650e109d578a39aba2074a5bf9a0bc6453ea5},
abstract = {LLM-based formal proof assistants (e.g., in Lean) hold great promise for automating mathematical discovery. But beyond syntactic correctness, do these systems truly understand mathematical structure as humans do? We investigate this question through the lens of mathematical inequalities -- a fundamental tool across many domains. While modern provers can solve basic inequalities, we probe their ability to handle human-intuitive compositionality. We introduce Ineq-Comp, a benchmark built from elementary inequalities through systematic transformations, including variable duplication, algebraic rewriting, and multi-step composition. Although these problems remain easy for humans, we find that most provers -- including Goedel, STP, and Kimina-7B -- struggle significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly because it is trained to decompose the problems into sub-problems -- but still suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor for all models even when formal proofs of the constituent parts are provided in context, revealing that the source of weakness is indeed in compositional reasoning. Our results expose a persisting gap between the generalization behavior of current AI provers and human mathematical intuition.},
author = {Haoyu Zhao and Yihan Geng and Shange Tang and Yong Lin and Bohan Lyu and Hongzhou Lin and Chi Jin and Sanjeev Arora},
journal = {ArXiv},
volume = {abs/2505.12680},
pages = {null},
doi = {10.48550/arXiv.2505.12680},
arxivid = {2505.12680},
}

@article{058ce15272a76a1c6376e7987b28644067f1ef92,
title = {SubgoalXL: Subgoal-based Expert Learning for Theorem Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/058ce15272a76a1c6376e7987b28644067f1ef92},
abstract = {Formal theorem proving, a field at the intersection of mathematics and computer science, has seen renewed interest with advancements in large language models (LLMs). This paper introduces SubgoalXL, a novel approach that synergizes subgoal-based proofs with expert learning to enhance LLMs' capabilities in formal theorem proving within the Isabelle environment. SubgoalXL addresses two critical challenges: the scarcity of specialized mathematics and theorem-proving data, and the need for improved multi-step reasoning abilities in LLMs. By optimizing data efficiency and employing subgoal-level supervision, SubgoalXL extracts richer information from limited human-generated proofs. The framework integrates subgoal-oriented proof strategies with an expert learning system, iteratively refining formal statement, proof, and subgoal generators. Leveraging the Isabelle environment's advantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art performance of 56.1\% in Isabelle on the standard miniF2F dataset, marking an absolute improvement of 4.9\%. Notably, SubgoalXL successfully solves 41 AMC12, 9 AIME, and 3 IMO problems from miniF2F. These results underscore the effectiveness of maximizing limited data utility and employing targeted guidance for complex reasoning in formal theorem proving, contributing to the ongoing advancement of AI reasoning capabilities. The implementation is available at \url{https://github.com/zhaoxlpku/SubgoalXL}.},
author = {Xueliang Zhao and Lin Zheng and Haige Bo and Changran Hu and Urmish Thakker and Lingpeng Kong},
journal = {ArXiv},
volume = {abs/2408.11172},
pages = {null},
doi = {10.48550/arXiv.2408.11172},
arxivid = {2408.11172},
}

@article{eda90d3d4f8515b778e8255399c1bd950009c1a0,
title = {ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data},
year = {2025},
url = {https://www.semanticscholar.org/paper/eda90d3d4f8515b778e8255399c1bd950009c1a0},
abstract = {Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 117k theorem statements and develop ATLAS Translator, which demonstrates statistically significant improvements over both the HERALD Translator and the Kimina-Autoformalizer across all benchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The datasets, model, and code will be released to the public soon.},
author = {Xiaoyang Liu and Kangjie Bao and Jiashuo Zhang and Yunqi Liu and Yu Chen and Yuntian Liu and Yang Jiao and Tao Luo},
journal = {ArXiv},
volume = {abs/2502.05567},
pages = {null},
doi = {10.48550/arXiv.2502.05567},
arxivid = {2502.05567},
}

@article{3393fa3da068bbd04af6e7716ec3f60e688af1b0,
title = {Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/3393fa3da068bbd04af6e7716ec3f60e688af1b0},
abstract = {Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7\%, 32.8\%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \texttt{imo\_2019\_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced.},
author = {Chenrui Cao and Liangcheng Song and Zenan Li and Xinyi Le and Xian Zhang and Hui Xue and Fan Yang},
journal = {ArXiv},
volume = {abs/2506.11487},
pages = {null},
doi = {10.48550/arXiv.2506.11487},
arxivid = {2506.11487},
}

@article{6ef5a9e475c3596cb42a4e320b9f0a210e06c0f4,
title = {CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics},
year = {2025},
url = {https://www.semanticscholar.org/paper/6ef5a9e475c3596cb42a4e320b9f0a210e06c0f4},
abstract = {Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both ``with solution'' and ``without solution'' scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at https://github.com/MoonshotAI/CombiBench/.},
author = {Junqi Liu and Xiaohan Lin and Jonas Bayer and Yael Dillies and Weijie Jiang and Xiaodan Liang and Roman Soletskyi and Haiming Wang and Yunzhou Xie and Beibei Xiong and Zheng-Sheng Yang and Jujian Zhang and Lihong Zhi and Jia Li and Zhengying Liu},
journal = {ArXiv},
volume = {abs/2505.03171},
pages = {null},
doi = {10.48550/arXiv.2505.03171},
arxivid = {2505.03171},
}

@article{52bbfafbc63866d95015027ae17595fc3396edb0,
title = {LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation},
year = {2025},
url = {https://www.semanticscholar.org/paper/52bbfafbc63866d95015027ae17595fc3396edb0},
abstract = {Recent advancements in large language models (LLMs) have sparked considerable interest in automated theorem proving and a prominent line of research integrates stepwise LLM-based provers into tree search. In this paper, we introduce a novel proof-state exploration approach for training data synthesis, designed to produce diverse tactics across a wide range of intermediate proof states, thereby facilitating effective one-shot fine-tuning of LLM as the policy model. We also propose an adaptive beam size strategy, which effectively takes advantage of our data synthesis method and achieves a trade-off between exploration and exploitation during tree search. Evaluations on the MiniF2F and ProofNet benchmarks demonstrate that our method outperforms strong baselines under the stringent Pass@1 metric, attaining an average pass rate of $60.74\%$ on MiniF2F and $21.18\%$ on ProofNet. These results underscore the impact of large-scale synthetic data in advancing automated theorem proving.},
author = {Junyu Lai and Jiakun Zhang and Shuo Xu and Taolue Chen and Zihang Wang and Yao Yang and Jiarui Zhang and Chun Cao and Jingwei Xu},
journal = {ArXiv},
volume = {abs/2505.12031},
pages = {null},
doi = {10.48550/arXiv.2505.12031},
arxivid = {2505.12031},
}

@article{10f221d9c0ebaa8c5a0ea53fdec404bbda3c421c,
title = {ProofCompass: Enhancing Specialized Provers with LLM Guidance},
year = {2025},
url = {https://www.semanticscholar.org/paper/10f221d9c0ebaa8c5a0ea53fdec404bbda3c421c},
abstract = {Language models have become increasingly powerful tools for formal mathematical reasoning. However, most existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources. This paper introduces ProofCompass, a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods, such as DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without requiring additional model training. The LLM provides natural language proof strategies and analyzes failed attempts to select intermediate lemmas, enabling effective problem decomposition. On the miniF2F benchmark, ProofCompass demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\% \rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$). Our synergistic approach paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving.},
author = {Nicolas Wischermann and C. M. Verdun and Gabriel Poesia and Francesco Noseda},
journal = {ArXiv},
volume = {abs/2507.14335},
pages = {null},
doi = {10.48550/arXiv.2507.14335},
arxivid = {2507.14335},
}

@article{5cf103a9610d3f1bb11ec254281f2e01284819b1,
title = {Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction},
year = {2025},
url = {https://www.semanticscholar.org/paper/5cf103a9610d3f1bb11ec254281f2e01284819b1},
abstract = {We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.},
author = {Yong Lin and Shange Tang and Bohan Lyu and Ziran Yang and Jui-Hui Chung and Haoyu Zhao and Lai Jiang and Yihan Geng and Jiawei Ge and Jingruo Sun and Jiayun Wu and Jiri Gesi and Ximing Lu and David Acuna and Kaiyu Yang and Hongzhou Lin and Yejin Choi and Danqi Chen and Sanjeev Arora and Chi Jin},
arxivid = {2508.03613},
}

@article{5b3cd003ae273b5ae9c29259e1db1b67a97cfd4e,
title = {InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems},
year = {2024},
url = {https://www.semanticscholar.org/paper/5b3cd003ae273b5ae9c29259e1db1b67a97cfd4e},
abstract = {Large Language Models (LLMs) have emerged as powerful tools in mathematical theorem proving, particularly when utilizing formal languages such as LEAN. The major learning paradigm is expert iteration, which necessitates a pre-defined dataset comprising numerous mathematical problems. In this process, LLMs attempt to prove problems within the dataset and iteratively refine their capabilities through self-training on the proofs they discover. We propose to use large scale LEAN problem datasets Lean-workbook for expert iteration with more than 20,000 CPU days. During expert iteration, we found log-linear trends between solved problem amount with proof length and CPU usage. We train a critic model to select relatively easy problems for policy models to make trials and guide the model to search for deeper proofs. InternLM2.5-StepProver achieves open-source state-of-the-art on MiniF2F, Lean-Workbook-Plus, ProofNet, and Putnam benchmarks. Specifically, it achieves a pass of 65.9% on the MiniF2F-test and proves (or disproves) 17.0% of problems in Lean-Workbook-Plus which shows a significant improvement compared to only 9.5% of problems proved when Lean-Workbook-Plus was released. We open-source our models and searched proofs at https://github.com/InternLM/InternLM-Math and https://huggingface.co/datasets/internlm/Lean-Workbook.},
author = {Zijian Wu and Suozhi Huang and Zhejian Zhou and Huaiyuan Ying and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2410.15700},
pages = {null},
doi = {10.48550/arXiv.2410.15700},
arxivid = {2410.15700},
}

@article{a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68,
title = {Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68},
abstract = {We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art (as of April 5 2025) performance in automated formal proof generation for mathematical problems. A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches. First, we train LLMs to convert natural language math problems from the Numina dataset to equivalent formal statements in Lean 4. This process creates the dataset Goedel-Pset-v1, which includes 1.64 million formal statements. Next, we develop a large dataset of formal proofs by training a series of provers. Each new prover can prove many statements that previous ones could not, and these new proofs are added to the training set for the next prover. Finally, we obtain the dataset Goedel-Pset-v1-solved, which contains proofs for over 800K statements from Goedel-Pset-v1. Supervised fine-tuning (SFT) of DeepSeek-Prover-V1.5-Base on Goedel-Pset-v1-solved (i.e., no RL) yields a Goedel-Prover-SFT that achieves a success rate of 57.6% (Pass@32) on miniF2F, surpassing the previous leader DeepSeek-Prover-V1.5-RL (trained using SFT + RL on a proprietary dataset) by 7.6%. On PutnamBench, Goedel-Prover-SFT successfully solves 7 problems (Pass@512), ranking first on the leaderboard. We provide extensive discussion of our training methodology, highlighting the key design choices that contribute to Goedel-Prover's strong performance. Further RL training (including DPO) improves Goedel-Prover-SFT's success rate to over 60% (Pass@32) on miniF2F. To aid future research, we provide extensive discussion of our training methodology and design choices. We also fully open-source our codes, models, and datasets. Additionally, we open-source formal proofs for 29.7K problems in Lean Workbook, nearly doubling the 15.7K solved by prior provers.},
author = {Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},
journal = {ArXiv},
volume = {abs/2502.07640},
pages = {null},
doi = {10.48550/arXiv.2502.07640},
arxivid = {2502.07640},
}

@article{f9c2e229d439021583c1e08da6173c686b5b218e,
title = {Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs},
year = {2025},
url = {https://www.semanticscholar.org/paper/f9c2e229d439021583c1e08da6173c686b5b218e},
abstract = {This position paper provides a critical but constructive discussion of current practices in benchmarking and evaluative practices in the field of formal reasoning and automated theorem proving. We take the position that open code, open data, and benchmarks that are complete and error-free will accelerate progress in this field. We identify practices that create barriers to contributing to this field and suggest ways to remove them. We also discuss some of the practices that might produce misleading evaluative information. We aim to create discussions that bring together people from various groups contributing to automated theorem proving, autoformalization, and informal reasoning.},
author = {Roozbeh Yousefzadeh and Xuenan Cao},
journal = {ArXiv},
volume = {abs/2507.04719},
pages = {null},
doi = {10.48550/arXiv.2507.04719},
arxivid = {2507.04719},
}
