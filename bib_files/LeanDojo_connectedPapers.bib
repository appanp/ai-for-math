@article{87875a07976c26f82705de1fc70041169e5d652b,
title = {LeanDojo: Theorem Proving with Retrieval-Augmented Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/87875a07976c26f82705de1fc70041169e5d652b},
abstract = {Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection: a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.},
author = {Kaiyu Yang and Aidan M. Swope and Alex Gu and Rahul Chalamala and Peiyang Song and Shixing Yu and Saad Godil and R. Prenger and Anima Anandkumar},
journal = {ArXiv},
volume = {abs/2306.15626},
pages = {null},
doi = {10.48550/arXiv.2306.15626},
arxivid = {2306.15626},
}

@article{473ec8d855adc1dbbfbb562d0a7c425a45b28e06,
title = {3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes},
year = {2024},
url = {https://www.semanticscholar.org/paper/473ec8d855adc1dbbfbb562d0a7c425a45b28e06},
abstract = {A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success, and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D- Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F and LeanDojo benchmarks by augmenting popular open source proving LLMs. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity. We make our code available at https://github.com/sean-lamont/3D-Prover.},
author = {Sean Lamont and Christian Walder and Amir Dezfouli and Paul Montague and Michael Norrish},
journal = {ArXiv},
volume = {abs/2410.11133},
pages = {null},
doi = {10.48550/arXiv.2410.11133},
arxivid = {2410.11133},
}

@article{7ba98b00a224094c09676090f5d6d69498f5b299,
title = {MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics},
year = {2021},
url = {https://www.semanticscholar.org/paper/7ba98b00a224094c09676090f5d6d69498f5b299},
abstract = {We present miniF2F, a dataset of formal Olympiad-level mathematics problems statements intended to provide a unified cross-system benchmark for neural theorem proving. The miniF2F benchmark currently targets Metamath, Lean, Isabelle (partially) and HOL Light (partially) and consists of 488 problem statements drawn from the AIME, AMC, and the International Mathematical Olympiad (IMO), as well as material from high-school and undergraduate mathematics courses. We report baseline results using GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of its performance. We intend for miniF2F to be a community-driven effort and hope that our benchmark will help spur advances in neural theorem proving.},
author = {Kunhao Zheng and Jesse Michael Han and Stanislas Polu},
journal = {ArXiv},
volume = {abs/2109.00110},
pages = {null},
arxivid = {2109.00110},
}

@article{9f8ac6ee3760ab202e492c733362e5bfc6763934,
title = {Baldur: Whole-Proof Generation and Repair with Large Language Models},
year = {2023},
url = {https://www.semanticscholar.org/paper/9f8ac6ee3760ab202e492c733362e5bfc6763934},
abstract = {Formally verifying software is a highly desirable but labor-intensive task. Recent work has developed methods to automate formal verification using proof assistants, such as Coq and Isabelle/HOL, e.g., by training a model to predict one proof step at a time and using that model to search through the space of possible proofs. This paper introduces a new method to automate formal verification: We use large language models, trained on natural language and code and fine-tuned on proofs, to generate whole proofs at once. We then demonstrate that a model fine-tuned to repair generated proofs further increasing proving power. This paper: (1) Demonstrates that whole-proof generation using transformers is possible and is as effective but more efficient than search-based techniques. (2) Demonstrates that giving the learned model additional context, such as a prior failed proof attempt and the ensuing error message, results in proof repair that further improves automated proof generation. (3) Establishes, together with prior work, a new state of the art for fully automated proof synthesis. We reify our method in a prototype, Baldur, and evaluate it on a benchmark of 6,336 Isabelle/HOL theorems and their proofs, empirically showing the effectiveness of whole-proof generation, repair, and added context. We also show that Baldur complements the state-of-the-art tool, Thor, by automatically generating proofs for an additional 8.7% of the theorems. Together, Baldur and Thor can prove 65.7% of the theorems fully automatically. This paper paves the way for new research into using large language models for automating formal verification.},
author = {E. First and M. Rabe and T. Ringer and Yuriy Brun},
journal = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
volume = {null},
pages = {null},
doi = {10.1145/3611643.3616243},
arxivid = {2303.04910},
}

@article{c4c0d6ffd70081d143b32be53b06fec1259b3ad8,
title = {The Lean 4 Theorem Prover and Programming Language},
year = {2021},
url = {https://www.semanticscholar.org/paper/c4c0d6ffd70081d143b32be53b06fec1259b3ad8},
abstract = {Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.},
author = {L. D. Moura and Sebastian Ullrich},
doi = {10.1007/978-3-030-79876-5_37},
}

@article{916a06a6d51aa93de27aac2f3e14faed08dd6706,
title = {Formal Mathematics Statement Curriculum Learning},
year = {2022},
url = {https://www.semanticscholar.org/paper/916a06a6d51aa93de27aac2f3e14faed08dd6706},
abstract = {We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we achieve state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.},
author = {Stanislas Polu and Jesse Michael Han and Kunhao Zheng and Mantas Baksys and Igor Babuschkin and I. Sutskever},
journal = {ArXiv},
volume = {abs/2202.01344},
pages = {null},
arxivid = {2202.01344},
}

@article{29d2bf6d4b0ce5cdd2cf0ad3103597ba5681f29f,
title = {Proving Theorems Recursively},
year = {2024},
url = {https://www.semanticscholar.org/paper/29d2bf6d4b0ce5cdd2cf0ad3103597ba5681f29f},
abstract = {Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation. However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs. To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover. Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture. Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels. This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels. Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods. POETRY on miniF2F achieves an average proving success rate improvement of 5.1%. Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26.},
author = {Haiming Wang and Huajian Xin and Zhengying Liu and Wenda Li and Yinya Huang and Jianqiao Lu and Zhicheng YANG and Jing Tang and Jian Yin and Zhenguo Li and Xiaodan Liang},
journal = {ArXiv},
volume = {abs/2405.14414},
pages = {null},
doi = {10.48550/arXiv.2405.14414},
arxivid = {2405.14414},
}

@article{217ea09e54a80043ec06d7f5ec1e7f2f7eef5b43,
title = {FIMO: A Challenge Formal Dataset for Automated Theorem Proving},
year = {2023},
url = {https://www.semanticscholar.org/paper/217ea09e54a80043ec06d7f5ec1e7f2f7eef5b43},
abstract = {We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes.},
author = {Chengwu Liu and Jianhao Shen and Huajian Xin and Zhengying Liu and Ye Yuan and Haiming Wang and Wei Ju and Chuanyang Zheng and Yichun Yin and Lin Li and Ming Zhang and Qun Liu},
journal = {ArXiv},
volume = {abs/2309.04295},
pages = {null},
doi = {10.48550/arXiv.2309.04295},
arxivid = {2309.04295},
}

@article{3648515cc35b517cdf60331cc4870e24616f9939,
title = {DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
year = {2024},
url = {https://www.semanticscholar.org/paper/3648515cc35b517cdf60331cc4870e24616f9939},
abstract = {Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.},
author = {Huajian Xin and Daya Guo and Zhihong Shao and Z. Ren and Qihao Zhu and Bo Liu (Benjamin Liu) and C. Ruan and Wenda Li and Xiaodan Liang},
journal = {ArXiv},
volume = {abs/2405.14333},
pages = {null},
doi = {10.48550/arXiv.2405.14333},
arxivid = {2405.14333},
}

@article{7899f3ec633080ac9d9b6458f1e1c35e86e6ec5c,
title = {Formal Mathematical Reasoning: A New Frontier in AI},
year = {2024},
url = {https://www.semanticscholar.org/paper/7899f3ec633080ac9d9b6458f1e1c35e86e6ec5c},
abstract = {AI for Mathematics (AI4Math) is not only intriguing intellectually but also crucial for AI-driven discovery in science, engineering, and beyond. Extensive efforts on AI4Math have mirrored techniques in NLP, in particular, training large language models on carefully curated math datasets in text form. As a complementary yet less explored avenue, formal mathematical reasoning is grounded in formal systems such as proof assistants, which can verify the correctness of reasoning and provide automatic feedback. In this position paper, we advocate for formal mathematical reasoning and argue that it is indispensable for advancing AI4Math to the next level. In recent years, we have seen steady progress in using AI to perform formal reasoning, including core tasks such as theorem proving and autoformalization, as well as emerging applications such as verifiable generation of code and hardware designs. However, significant challenges remain to be solved for AI to truly master mathematics and achieve broader impact. We summarize existing progress, discuss open challenges, and envision critical milestones to measure future success. At this inflection point for formal mathematical reasoning, we call on the research community to come together to drive transformative advancements in this field.},
author = {Kaiyu Yang and Gabriel Poesia and Jingxuan He and Wenda Li and Kristin Lauter and Swarat Chaudhuri and D. Song},
journal = {ArXiv},
volume = {abs/2412.16075},
pages = {null},
doi = {10.48550/arXiv.2412.16075},
arxivid = {2412.16075},
}

@article{272afc28d03890160b1f2808cc551c962ea9138c,
title = {ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics},
year = {2023},
url = {https://www.semanticscholar.org/paper/272afc28d03890160b1f2808cc551c962ea9138c},
abstract = {We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation.},
author = {Zhangir Azerbayev and Bartosz Piotrowski and Hailey Schoelkopf and Edward W. Ayers and Dragomir R. Radev and J. Avigad},
journal = {ArXiv},
volume = {abs/2302.12433},
pages = {null},
doi = {10.48550/arXiv.2302.12433},
arxivid = {2302.12433},
}

@article{f8b5ee53c3410f20049e7def47bd52403fa388e3,
title = {LEGO-Prover: Neural Theorem Proving with Growing Libraries},
year = {2023},
url = {https://www.semanticscholar.org/paper/f8b5ee53c3410f20049e7def47bd52403fa388e3},
abstract = {Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%). During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We also release our code and all the generated skills.},
author = {Huajian Xin and Haiming Wang and Chuanyang Zheng and Lin Li and Zhengying Liu and Qingxing Cao and Yinya Huang and Jing Xiong and Han Shi and Enze Xie and Jian Yin and Zhenguo Li and Xiaodan Liang and Heng Liao},
journal = {ArXiv},
volume = {abs/2310.00656},
pages = {null},
doi = {10.48550/arXiv.2310.00656},
arxivid = {2310.00656},
}

@article{d4b542847e1dd227e90479f4b3523a81dee33b7a,
title = {Lyra: Orchestrating Dual Correction in Automated Theorem Proving},
year = {2023},
url = {https://www.semanticscholar.org/paper/d4b542847e1dd227e90479f4b3523a81dee33b7a},
abstract = {Large Language Models (LLMs) present an intriguing avenue for exploration in the field of formal theorem proving. Nevertheless, their full potential, particularly concerning the mitigation of hallucinations and refinement through prover error messages, remains an area that has yet to be thoroughly investigated. To enhance the effectiveness of LLMs in the field, we introduce the Lyra, a new framework that employs two distinct correction mechanisms: Tool Correction (TC) and Conjecture Correction (CC). To implement Tool Correction in the post-processing of formal proofs, we leverage prior knowledge to utilize predefined prover tools (e.g., Sledgehammer) for guiding the replacement of incorrect tools. Tool Correction significantly contributes to mitigating hallucinations, thereby improving the overall accuracy of the proof. In addition, we introduce Conjecture Correction, an error feedback mechanism designed to interact with prover to refine formal proof conjectures with prover error messages. Compared to the previous refinement framework, the proposed Conjecture Correction refines generation with instruction but does not collect paired (generation, error&refinement) prompts. Our method has achieved state-of-the-art (SOTA) performance on both miniF2F validation (48.0% ->55.3%) and test (45.5% ->51.2%). We also present 3 IMO problems solved by Lyra. We believe Tool Correction (post-process for hallucination mitigation) and Conjecture Correction (subgoal adjustment from interaction with environment) could provide a promising avenue for future research in this field.},
author = {Chuanyang Zheng and Haiming Wang and Enze Xie and Zhengying Liu and Jiankai Sun and Huajian Xin and Jianhao Shen and Zheng Li and Yu Li},
journal = {Trans. Mach. Learn. Res.},
volume = {2024},
pages = {null},
doi = {10.48550/arXiv.2309.15806},
arxivid = {2309.15806},
}

@article{593499b654360101682edec1dd711fa7c09f6971,
title = {IsarStep: a Benchmark for High-level Mathematical Reasoning},
year = {2021},
url = {https://www.semanticscholar.org/paper/593499b654360101682edec1dd711fa7c09f6971},
abstract = {S2 TL;DR: A benchmark for high-level mathematical reasoning is presented and the reasoning capabilities of neural sequence-to-sequence models are studied and a hierarchical transformer is designed that outperforms the transformer baseline.},
author = {Wenda Li and Lei Yu and Yuhuai Wu and Lawrence Charles Paulson},
}

@article{a230273e0f9f43c3fefb11b191b4e23ae21aec3e,
title = {DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function},
year = {2023},
url = {https://www.semanticscholar.org/paper/a230273e0f9f43c3fefb11b191b4e23ae21aec3e},
abstract = {Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration.Experiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65% improvement on average in terms of success rate. And especially under low computing resource settings (11.03% improvement on average).},
author = {Haiming Wang and Ye Yuan and Zhengying Liu and Jianhao Shen and Yichun Yin and Jing Xiong and Enze Xie and Han Shi and Yujun Li and Lin Li and Jian Yin and Zhenguo Li and Xiaodan Liang},
doi = {10.18653/v1/2023.acl-long.706},
}

@article{f661bdc053ba13df80eda479791306e1178db235,
title = {Magnushammer: A Transformer-based Approach to Premise Selection},
year = {2023},
url = {https://www.semanticscholar.org/paper/f661bdc053ba13df80eda479791306e1178db235},
abstract = {This paper presents a novel approach to premise selection, a crucial reasoning task in automated theorem proving. Traditionally, symbolic methods that rely on extensive domain knowledge and engineering effort are applied to this task. In contrast, this work demonstrates that contrastive training with the transformer architecture can achieve higher-quality retrieval of relevant premises, without the engineering overhead. Our method, Magnushammer, outperforms the most advanced and widely used automation tool in interactive theorem proving called Sledgehammer. On the PISA and miniF2F benchmarks Magnushammer achieves $59.5\%$ (against $38.3\%$) and $34.0\%$ (against $20.9\%$) success rates, respectively. By combining \method with a language-model-based automated theorem prover, we further improve the state-of-the-art proof success rate from $57.0\%$ to $71.0\%$ on the PISA benchmark using $4$x fewer parameters. Moreover, we develop and open source a novel dataset for premise selection, containing textual representations of (proof state, relevant premise) pairs. To the best of our knowledge, this is the largest available premise selection dataset, and the first one for the Isabelle proof assistant.},
author = {Maciej Mikuła and Szymon Antoniak and Szymon Tworkowski and Albert Qiaochu Jiang and Jinyi Zhou and Christian Szegedy and Lukasz Kuci'nski and Piotr Milo's and Yuhuai Wu},
journal = {ArXiv},
volume = {abs/2303.04488},
pages = {null},
doi = {10.48550/arXiv.2303.04488},
arxivid = {2303.04488},
}

@article{065112180cd381ffc018780cf8fc0a14ae2580b1,
title = {Proof Artifact Co-training for Theorem Proving with Language Models},
year = {2021},
url = {https://www.semanticscholar.org/paper/9231927bc0a9ed10de64cad05640587893eba4b1},
abstract = {Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime. We propose PACT ({\bf P}roof {\bf A}rtifact {\bf C}o-{\bf T}raining), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for co-training alongside the usual tactic prediction objective. We apply this methodology to Lean, an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32\% to 48\%.},
author = {Jesse Michael Han and Jason M. Rute and Yuhuai Wu and Edward W. Ayers and Stanislas Polu},
journal = {ArXiv},
volume = {abs/2102.06203},
pages = {null},
arxivid = {2102.06203},
}

@article{ee71447d68f3d8666c974b8199e330e19aebf263,
title = {An In-Context Learning Agent for Formal Theorem-Proving},
year = {2023},
url = {https://www.semanticscholar.org/paper/ee71447d68f3d8666c974b8199e330e19aebf263},
abstract = {We present an in-context learning agent for formal theorem-proving in environments like Lean and Coq. Current state-of-the-art models for the problem are finetuned on environment-specific proof data. By contrast, our approach, called COPRA, repeatedly asks a high-capacity, general-purpose large language model (GPT-4) to propose tactic applications from within a stateful backtracking search. Proposed tactics are executed in the underlying proof environment. Feedback from the execution is used to build the prompt for the next model query, along with selected information from the search history and lemmas retrieved from an external database. We evaluate our implementation of COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the CompCert project. On these benchmarks, COPRA significantly outperforms few-shot invocations of GPT-4. It also compares favorably against finetuning-based approaches, outperforming ReProver, a state-of-the-art finetuned approach for Lean, in terms of the pass@1 metric. Our code and data are available at https://github.com/trishullab/copra.},
author = {Amitayush Thakur and G. Tsoukalas and Yeming Wen and Jimmy Xin and Swarat Chaudhuri},
arxivid = {2310.04353},
}

@article{b8fe9d7b5762f7c9c3789cff2bdbe968ff0f0ed6,
title = {Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving},
year = {2023},
url = {https://www.semanticscholar.org/paper/b8fe9d7b5762f7c9c3789cff2bdbe968ff0f0ed6},
abstract = {Large language models~(LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodologies, we have successfully increased the prevailing proof accuracy from 38.9\% to 44.3\% on the miniF2F benchmark. Furthermore, the adoption of diffusion models for demonstration organization can lead to an additional enhancement in accuracy to 45.5\%, or a $5\times$ improvement in sampling efficiency compared with the long-standing state-of-the-art method. Our code is available at \url{https://github.com/HKUNLP/subgoal-theorem-prover}.},
author = {Xueliang Zhao and Wenda Li and Lingpeng Kong},
journal = {ArXiv},
volume = {abs/2305.16366},
pages = {null},
doi = {10.48550/arXiv.2305.16366},
arxivid = {2305.16366},
}

@article{c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c,
title = {Thor: Wielding Hammers to Integrate Language Models and Automated Theorem Provers},
year = {2022},
url = {https://www.semanticscholar.org/paper/c2d574f7c6a9e3bafe396ecb4ab639179d6fd92c},
abstract = {In theorem proving, the task of selecting useful premises from a large library to unlock the proof of a given conjecture is crucially important. This presents a challenge for all theorem provers, especially the ones based on language models, due to their relative inability to reason over huge volumes of premises in text form. This paper introduces Thor, a framework integrating language models and automated theorem provers to overcome this difficulty. In Thor, a class of methods called hammers that leverage the power of automated theorem provers are used for premise selection, while all other tasks are designated to language models. Thor increases a language model's success rate on the PISA dataset from $39\%$ to $57\%$, while solving $8.2\%$ of problems neither language models nor automated theorem provers are able to solve on their own. Furthermore, with a significantly smaller computational budget, Thor can achieve a success rate on the MiniF2F dataset that is on par with the best existing methods. Thor can be instantiated for the majority of popular interactive theorem provers via a straightforward protocol we provide.},
author = {Albert Qiaochu Jiang and Wenda Li and Szymon Tworkowski and K. Czechowski and Tomasz Odrzyg'o'zd'z and Piotr Milo's and Yuhuai Wu and M. Jamnik},
journal = {ArXiv},
volume = {abs/2205.10893},
pages = {null},
doi = {10.48550/arXiv.2205.10893},
arxivid = {2205.10893},
}

@article{1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3,
title = {Lean Workbook: A large-scale Lean problem set formalized from natural language math problems},
year = {2024},
url = {https://www.semanticscholar.org/paper/1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3},
abstract = {Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.},
author = {Huaiyuan Ying and Zijian Wu and Yihan Geng and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2406.03847},
pages = {null},
doi = {10.48550/arXiv.2406.03847},
arxivid = {2406.03847},
}

@article{a596f03145285cd05a6ca57a4e25418b23b24976,
title = {Learning to Prove Theorems via Interacting with Proof Assistants},
year = {2019},
url = {https://www.semanticscholar.org/paper/a596f03145285cd05a6ca57a4e25418b23b24976},
abstract = {Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at this https URL.},
author = {Kaiyu Yang and Jia Deng},
journal = {ArXiv},
volume = {abs/1905.09381},
pages = {null},
arxivid = {1905.09381},
}

@article{65b4b25272c50dc376f5c018338931bfd349e532,
title = {HyperTree Proof Search for Neural Theorem Proving},
year = {2022},
url = {https://www.semanticscholar.org/paper/65b4b25272c50dc376f5c018338931bfd349e532},
abstract = {We propose an online training procedure for a transformer-based automated theorem prover. Our approach leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero. Our model learns from previous proof searches through online training, allowing it to generalize to domains far from the training distribution. We report detailed ablations of our pipeline's main components by studying performance on three environments of increasing complexity. In particular, we show that with HTPS alone, a model trained on annotated proofs manages to prove 65.4% of a held-out set of Metamath theorems, significantly outperforming the previous state of the art of 56.5% by GPT-f. Online training on these unproved theorems increases accuracy to 82.6%. With a similar computational budget, we improve the state of the art on the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.},
author = {Guillaume Lample and M. Lachaux and Thibaut Lavril and Xavier Martinet and Amaury Hayat and Gabriel Ebner and Aur'elien Rodriguez and Timothée Lacroix},
journal = {ArXiv},
volume = {abs/2205.11491},
pages = {null},
doi = {10.48550/arXiv.2205.11491},
arxivid = {2205.11491},
}

@article{7ec4416f4017bb7c76df9b9ff313021bb2dca3ec,
title = {L YRA : O RCHESTRATING D UAL C ORRECTION IN A UTO - MATED T HEOREM P ROVING},
year = {2023},
url = {https://www.semanticscholar.org/paper/7ec4416f4017bb7c76df9b9ff313021bb2dca3ec},
abstract = {S2 TL;DR: This work demonstrated the feasibility and effectiveness of Lyra by reaching state-of-the-art performance on the miniF2F dataset validation and test, respectively, with the Isabelle theorem prover.},
author = {Chuanyang Zheng and Haiming Wang and Enze Xie and Zhengying Liu and Jiankai Sun and Huajian Xin and Jianhao Shen and Zhenguo Li and Yu Li},
}

@article{46b294941c397699fde0ee7e7fc441f6a755f671,
title = {D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS},
year = {2023},
url = {https://www.semanticscholar.org/paper/46b294941c397699fde0ee7e7fc441f6a755f671},
abstract = {S2 TL;DR: This work introduces Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems.},
author = {Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Guillaume Lample and Yuhuai Wu},
}

@article{91ca6535fc8fb03efe0ecbe424ce5354ed129b0c,
title = {Towards Large Language Models as Copilots for Theorem Proving in Lean},
year = {2024},
url = {https://www.semanticscholar.org/paper/91ca6535fc8fb03efe0ecbe424ce5354ed129b0c},
abstract = {S2 TL;DR: Lean Copilot is introduced, a framework for running neural network inference in Lean that enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users.},
author = {Peiyang Song and Kaiyu Yang and Anima Anandkumar},
journal = {ArXiv},
volume = {abs/2404.12534},
pages = {null},
doi = {10.48550/arXiv.2404.12534},
}

@article{45bb43cdc35324fea4350ed335c500d4a5fd6ef5,
title = {Mathematical Reasoning via Self-supervised Skip-tree Training},
year = {2020},
url = {https://www.semanticscholar.org/paper/45bb43cdc35324fea4350ed335c500d4a5fd6ef5},
abstract = {S2 TL;DR: It is found that models trained on the skip-tree task show surprisingly strong mathematical reasoning abilities, and outperform modelstrained on standard skip-sequence tasks.},
author = {M. Rabe and Dennis Lee and Kshitij Bansal and Christian Szegedy},
journal = {arXiv: Learning},
volume = {},
pages = {null},
}

@article{0eef63fe71efba4404a9a883bf89ec634dbaad82,
title = {Enhancing Neural Theorem Proving through Data Augmentation and Dynamic Sampling Method},
year = {2023},
url = {https://www.semanticscholar.org/paper/0eef63fe71efba4404a9a883bf89ec634dbaad82},
abstract = {Theorem proving is a fundamental task in mathematics. With the advent of large language models (LLMs) and interactive theorem provers (ITPs) like Lean, there has been growing interest in integrating LLMs and ITPs to automate theorem proving. In this approach, the LLM generates proof steps (tactics), and the ITP checks the applicability of the tactics at the current goal. The two systems work together to complete the proof. In this paper, we introduce DS-Prover, a novel dynamic sampling method for theorem proving. This method dynamically determines the number of tactics to apply to expand the current goal, taking into account the remaining time compared to the total allocated time for proving a theorem. This makes the proof search process more efficient by adjusting the balance between exploration and exploitation as time passes. We also augment the training dataset by decomposing simplification and rewrite tactics with multiple premises into tactics with single premises. This gives the model more examples to learn from and helps it to predict the tactics with premises more accurately. We perform our experiments using the Mathlib dataset of the Lean theorem prover and report the performance on two standard datasets, MiniF2F and ProofNet. Our methods achieve significant performance gains on both datasets. We achieved a state-of-the-art performance (Pass@1) of 14.2% on the ProofNet dataset and a performance of 29.8% on MiniF2F, slightly surpassing the best-reported Pass@1 of 29.6% using Lean.},
author = {Rahul Vishwakarma and Subhankar Mishra},
journal = {ArXiv},
volume = {abs/2312.14188},
pages = {null},
doi = {10.48550/arXiv.2312.14188},
arxivid = {2312.14188},
}

@article{85c4ada888e7c363e671a6398d8a7b99cf890317,
title = {LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover},
year = {2024},
url = {https://www.semanticscholar.org/paper/85c4ada888e7c363e671a6398d8a7b99cf890317},
abstract = {Recently, large language models have presented promising results in aiding formal mathematical reasoning. However, their performance is restricted due to the scarcity of formal theorem-proving data, which requires additional effort to be extracted from raw formal language corpora. Meanwhile, a significant amount of human-written formal language corpora remains underutilized. To address this issue, we propose LEAN-GitHub, a dataset consisting of large-scale formal data extracted from almost all Lean 4 repositories on GitHub. After fine-tuning InternLM-math-plus on this dataset, our model achieved accuracies of 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F test, surpassing state-of-the-art method at 52%. And it also achieves state-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting different fields/levels of math. These results demonstrate that our proposed dataset is beneficial for formal reasoning on a wide range of math topics. We open-source our model at https://GitHub. com/InternLM/InternLM-Math and our data at https://huggingface.co/ datasets/InternLM/Lean-GitHub},
author = {Zijian Wu and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2407.17227},
pages = {null},
doi = {10.48550/arXiv.2407.17227},
arxivid = {2407.17227},
}

@article{58c74cec28f3416b9a1d308bb2d6519d21d53ab0,
title = {LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning},
year = {2021},
url = {https://www.semanticscholar.org/paper/58c74cec28f3416b9a1d308bb2d6519d21d53ab0},
abstract = {While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce's view that deduction, induction, and abduction are the primitives of reasoning, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called"LIME"(Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task. The code for generating LIME tasks is available at https://github.com/tonywu95/LIME.},
author = {Yuhuai Wu and M. Rabe and Wenda Li and Jimmy Ba and R. Grosse and Christian Szegedy},
arxivid = {2101.06223},
}

@article{5fe0a4af3bd1479d5e39fbda2215c86bce54722b,
title = {Generative Language Modeling for Automated Theorem Proving},
year = {2020},
url = {https://www.semanticscholar.org/paper/5fe0a4af3bd1479d5e39fbda2215c86bce54722b},
abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
author = {Stanislas Polu and I. Sutskever},
journal = {ArXiv},
volume = {abs/2009.03393},
pages = {null},
arxivid = {2009.03393},
}

@article{a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68,
title = {Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68},
abstract = {We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art (as of April 5 2025) performance in automated formal proof generation for mathematical problems. A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches. First, we train LLMs to convert natural language math problems from the Numina dataset to equivalent formal statements in Lean 4. This process creates the dataset Goedel-Pset-v1, which includes 1.64 million formal statements. Next, we develop a large dataset of formal proofs by training a series of provers. Each new prover can prove many statements that previous ones could not, and these new proofs are added to the training set for the next prover. Finally, we obtain the dataset Goedel-Pset-v1-solved, which contains proofs for over 800K statements from Goedel-Pset-v1. Supervised fine-tuning (SFT) of DeepSeek-Prover-V1.5-Base on Goedel-Pset-v1-solved (i.e., no RL) yields a Goedel-Prover-SFT that achieves a success rate of 57.6% (Pass@32) on miniF2F, surpassing the previous leader DeepSeek-Prover-V1.5-RL (trained using SFT + RL on a proprietary dataset) by 7.6%. On PutnamBench, Goedel-Prover-SFT successfully solves 7 problems (Pass@512), ranking first on the leaderboard. We provide extensive discussion of our training methodology, highlighting the key design choices that contribute to Goedel-Prover's strong performance. Further RL training (including DPO) improves Goedel-Prover-SFT's success rate to over 60% (Pass@32) on miniF2F. To aid future research, we provide extensive discussion of our training methodology and design choices. We also fully open-source our codes, models, and datasets. Additionally, we open-source formal proofs for 29.7K problems in Lean Workbook, nearly doubling the 15.7K solved by prior provers.},
author = {Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},
journal = {ArXiv},
volume = {abs/2502.07640},
pages = {null},
doi = {10.48550/arXiv.2502.07640},
arxivid = {2502.07640},
}

@article{6a4501fefaf73261dc180ff86b52208679f3fb9c,
title = {A Survey on Deep Learning for Theorem Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/6a4501fefaf73261dc180ff86b52208679f3fb9c},
abstract = {Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, inspiring and catalyzing further research endeavors in this rapidly growing field. A curated list of papers is available at https://github.com/zhaoyu-li/DL4TP.},
author = {Zhaoyu Li and Jialiang Sun and Logan Murphy and Qidong Su and Zenan Li and Xian Zhang and Kaiyu Yang and Xujie Si},
journal = {ArXiv},
volume = {abs/2404.09939},
pages = {null},
doi = {10.48550/arXiv.2404.09939},
arxivid = {2404.09939},
}

@article{a3971439fe1f5d5cca6113bc4fa612f71ce7a7df,
title = {Lean-STaR: Learning to Interleave Thinking and Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/a3971439fe1f5d5cca6113bc4fa612f71ce7a7df},
abstract = {Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR achieves state-of-the-art results on the miniF2F-test benchmark within the Lean theorem proving environment, significantly outperforming base models ($\boldsymbol{43.4\% \rightarrow 46.3\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness.},
author = {Haohan Lin and Zhiqing Sun and Yiming Yang and S. Welleck},
journal = {ArXiv},
volume = {abs/2407.10040},
pages = {null},
doi = {10.48550/arXiv.2407.10040},
arxivid = {2407.10040},
}

@article{c28e95a06dfcf13fc65a1cac83722f53e34f12a5,
title = {Autoformalization with Large Language Models},
year = {2022},
url = {https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
abstract = {Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.},
author = {Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
journal = {ArXiv},
volume = {abs/2205.12615},
pages = {null},
doi = {10.48550/arXiv.2205.12615},
arxivid = {2205.12615},
}

@article{668341051f3c9c087e42e393c610792df3e45992,
title = {Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean},
year = {2024},
url = {https://www.semanticscholar.org/paper/668341051f3c9c087e42e393c610792df3e45992},
abstract = {Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.},
author = {Peiyang Song and Kaiyu Yang and Anima Anandkumar},
arxivid = {2404.12534},
}

@article{d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8,
title = {ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems},
year = {2022},
url = {https://www.semanticscholar.org/paper/d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8},
abstract = {S2 TL;DR: The introduced ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics, consists of 297 theorem statements expressed in both natural language and the Lean 3 theorem prover, 100 of which are also accompanied by natural language proofs.},
author = {Zhangir Azerbayev and Bartosz Piotrowski and J. Avigad},
}

@article{b16c7d45183b9d595ab64301be019741b1528860,
title = {Llemma: An Open Language Model For Mathematics},
year = {2023},
url = {https://www.semanticscholar.org/paper/b16c7d45183b9d595ab64301be019741b1528860},
abstract = {We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.},
author = {Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and S. McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and S. Welleck},
journal = {ArXiv},
volume = {abs/2310.10631},
pages = {null},
doi = {10.48550/arXiv.2310.10631},
arxivid = {2310.10631},
}

@article{6354569d60b80c85b7bd557b80e6d9a5cf719d6e,
title = {PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition},
year = {2024},
url = {https://www.semanticscholar.org/paper/6354569d60b80c85b7bd557b80e6d9a5cf719d6e},
abstract = {We present PutnamBench, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PutnamBench requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PutnamBench to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PutnamBench problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PutnamBench is available at https://github.com/trishullab/PutnamBench.},
author = {G. Tsoukalas and Jasper Lee and J. Jennings and Jimmy Xin and Michelle Ding and Michael Jennings and Amitayush Thakur and Swarat Chaudhuri},
journal = {ArXiv},
volume = {abs/2407.11214},
pages = {null},
doi = {10.48550/arXiv.2407.11214},
arxivid = {2407.11214},
}

@article{4634362d75606287955260ef1788171286efbeaa,
title = {Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
year = {2022},
url = {https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
abstract = {The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.},
author = {Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
journal = {ArXiv},
volume = {abs/2210.12283},
pages = {null},
doi = {10.48550/arXiv.2210.12283},
arxivid = {2210.12283},
}

@article{e1a642026fb46a8b8a868862bcf0728e8d215d7e,
title = {DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search},
year = {2024},
url = {https://www.semanticscholar.org/paper/e1a642026fb46a8b8a868862bcf0728e8d215d7e},
abstract = {We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark ($63.5\%$) and the undergraduate level ProofNet benchmark ($25.3\%$).},
author = {Huajian Xin and Z. Ren and Jun-Mei Song and Zhihong Shao and Wanjia Zhao and Haocheng Wang and Bo Liu (Benjamin Liu) and Liyue Zhang and Xuan Lu and Qiushi Du and Wenjun Gao and Qihao Zhu and Dejian Yang and Zhibin Gou and Z. F. Wu and Fuli Luo and C. Ruan},
journal = {ArXiv},
volume = {abs/2408.08152},
pages = {null},
doi = {10.48550/arXiv.2408.08152},
arxivid = {2408.08152},
}
