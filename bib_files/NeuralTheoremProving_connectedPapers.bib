@article{11ab53416b97f94df7d38df80b01d11a228bc6c3,
title = {Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification},
year = {2025},
url = {https://www.semanticscholar.org/paper/11ab53416b97f94df7d38df80b01d11a228bc6c3},
abstract = {Formally verifying properties of software code has been a highly desirable task, especially with the emergence of LLM-generated code. In the same vein, they provide an interesting avenue for the exploration of formal verification and mechanistic interpretability. Since the introduction of code-specific models, despite their successes in generating code in Lean4 and Isabelle, the task of generalized theorem proving still remains far from being fully solved and will be a benchmark for reasoning capability in LLMs. In this work, we introduce a framework that generates whole proofs in a formal language to be used within systems that utilize the power of built-in tactics and off-the-shelf automated theorem provers. Our framework includes 3 components: generating natural language statements of the code to be verified, an LLM that generates formal proofs for the given statement, and a module employing heuristics for building the final proof. To train the LLM, we employ a 2-stage fine-tuning process, where we first use SFT-based training to enable the model to generate syntactically correct Isabelle code and then RL-based training that encourages the model to generate proofs verified by a theorem prover. We validate our framework using the miniF2F-test benchmark and the Isabelle proof assistant and design a use case to verify the correctness of the AWS S3 bucket access policy code. We also curate a dataset based on the FVEL\textsubscript{\textnormal{ER}} dataset for future training tasks.},
author = {Balaji Rao and William Eiers and Carlo Lipizzi},
journal = {ArXiv},
volume = {abs/2504.17017},
pages = {null},
doi = {10.48550/arXiv.2504.17017},
arxivid = {2504.17017},
}

@article{3648515cc35b517cdf60331cc4870e24616f9939,
title = {DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
year = {2024},
url = {https://www.semanticscholar.org/paper/3648515cc35b517cdf60331cc4870e24616f9939},
abstract = {Proof assistants like Lean have revolutionized mathematical proof verification, ensuring high accuracy and reliability. Although large language models (LLMs) show promise in mathematical reasoning, their advancement in formal theorem proving is hindered by a lack of training data. To address this issue, we introduce an approach to generate extensive Lean 4 proof data derived from high-school and undergraduate-level mathematical competition problems. This approach involves translating natural language problems into formal statements, filtering out low-quality statements, and generating proofs to create synthetic data. After fine-tuning the DeepSeekMath 7B model on this synthetic dataset, which comprises 8 million formal statements with proofs, our model achieved whole-proof generation accuracies of 46.3% with 64 samples and 52% cumulatively on the Lean 4 miniF2F test, surpassing the baseline GPT-4 at 23.0% with 64 samples and a tree search reinforcement learning method at 41.0%. Additionally, our model successfully proved 5 out of 148 problems in the Lean 4 Formalized International Mathematical Olympiad (FIMO) benchmark, while GPT-4 failed to prove any. These results demonstrate the potential of leveraging large-scale synthetic data to enhance theorem-proving capabilities in LLMs. Both the synthetic dataset and the model will be made available to facilitate further research in this promising field.},
author = {Huajian Xin and Daya Guo and Zhihong Shao and Z. Ren and Qihao Zhu and Bo Liu (Benjamin Liu) and C. Ruan and Wenda Li and Xiaodan Liang},
journal = {ArXiv},
volume = {abs/2405.14333},
pages = {null},
doi = {10.48550/arXiv.2405.14333},
arxivid = {2405.14333},
}

@article{bef31c928031d0408d1f00c04a07921aef66fff0,
title = {miniCTX: Neural Theorem Proving with (Long-)Contexts},
year = {2024},
url = {https://www.semanticscholar.org/paper/bef31c928031d0408d1f00c04a07921aef66fff0},
abstract = {Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce miniCTX, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. miniCTX contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for miniCTX, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as miniF2F. Alongside miniCTX, we offer ntp-toolkit for automatically extracting and annotating theorem proving data, making it easy to add new projects into miniCTX to ensure that contexts are not seen during training. miniCTX offers a challenging and realistic evaluation of neural theorem provers.},
author = {Jiewen Hu and Thomas (Hanwen) Zhu and S. Welleck},
journal = {ArXiv},
volume = {abs/2408.03350},
pages = {null},
doi = {10.48550/arXiv.2408.03350},
arxivid = {2408.03350},
}

@article{f9e701ac5d025581f519eae1216e26475e56462b,
title = {STP: Self-play LLM Theorem Provers with Iterative Conjecturing and Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/f9e701ac5d025581f519eae1216e26475e56462b},
abstract = {A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 51.3 billion tokens generated during the training in Lean, STP proves 28.5% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (65.0%, pass@3200), Proofnet-test (23.9%, pass@3200) and PutnamBench (8/644, pass@3200). We release our code, model, and dataset in this URL: https://github.com/kfdong/STP.},
author = {Kefan Dong and Tengyu Ma},
journal = {ArXiv},
volume = {abs/2502.00212},
pages = {null},
doi = {10.48550/arXiv.2502.00212},
arxivid = {2502.00212},
}

@article{d6fa3cfde46c45d746853b39d7cc420ec96d8f97,
title = {Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning},
year = {2025},
url = {https://www.semanticscholar.org/paper/d6fa3cfde46c45d746853b39d7cc420ec96d8f97},
abstract = {We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover},
author = {Haiming Wang and Mert Unsal and Xiaohan Lin and Mantas Baksys and Junqi Liu and Marco Dos Santos and Flood Sung and Marina Vinyes and Zhenzhe Ying and Zekai Zhu and Jianqiao Lu and Hugues de Saxc'e and Bolton Bailey and Chendong Song and Chenjun Xiao and Dehao Zhang and Ebony Zhang and Frederick Pu and Han Zhu and Jiawei Liu and Jonas Bayer and Julien Michel and Long Yu and L. Dreyfus-Schmidt and Lewis Tunstall and L. Pagani and Moreira Machado and Pauline Bourigault and Ran Wang and Stanislas Polu and Thibaut Barroyer and Wen-Ding Li and Yazhe Niu and Y. Fleureau and Yang Hu and Zhouliang Yu and Zihan Wang and Zhilin Yang and Zhengying Liu and Jia Li},
journal = {ArXiv},
volume = {abs/2504.11354},
pages = {null},
doi = {10.48550/arXiv.2504.11354},
arxivid = {2504.11354},
}

@article{b308f9f7e60e04c33319f7459993bb33d5aff763,
title = {Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/b308f9f7e60e04c33319f7459993bb33d5aff763},
abstract = {Automated Theorem Proving (ATP) in formal languages is a foundational challenge for AI. While Large Language Models (LLMs) have driven remarkable progress, a significant gap remains between their powerful informal reasoning capabilities and their weak formal proving performance. Recent studies show that the informal accuracy exceeds 80% while formal success remains below 8% on benchmarks like PutnamBench. We argue this gap persists because current state-of-the-art provers, by tightly coupling reasoning and proving, are trained with paradigms that inadvertently punish deep reasoning in favor of shallow, tactic-based strategies. To bridge this fundamental gap, we propose a novel framework that decouples high-level reasoning from low-level proof generation. Our approach utilizes two distinct, specialized models: a powerful, general-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an efficient Prover to rigorously verify them. This modular design liberates the model's full reasoning potential and bypasses the pitfalls of end-to-end training. We evaluate our method on a challenging set of post-2000 IMO problems, a problem set on which no prior open-source prover has reported success. Our decoupled framework successfully solves 5 of these problems, demonstrating a significant step towards automated reasoning on exceptionally difficult mathematical challenges. To foster future research, we release our full dataset of generated and verified lemmas for a wide range of IMO problems, available at https://tencent-imo.github.io/ .},
author = {Zhenwen Liang and Linfeng Song and Yang Li and Tao Yang and Feng Zhang and Haitao Mi and Dong Yu},
journal = {ArXiv},
volume = {abs/2507.06804},
pages = {null},
doi = {10.48550/arXiv.2507.06804},
arxivid = {2507.06804},
}

@article{a3971439fe1f5d5cca6113bc4fa612f71ce7a7df,
title = {Lean-STaR: Learning to Interleave Thinking and Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/a3971439fe1f5d5cca6113bc4fa612f71ce7a7df},
abstract = {Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR achieves state-of-the-art results on the miniF2F-test benchmark within the Lean theorem proving environment, significantly outperforming base models ($\boldsymbol{43.4\% \rightarrow 46.3\%,}$ Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness.},
author = {Haohan Lin and Zhiqing Sun and Yiming Yang and S. Welleck},
journal = {ArXiv},
volume = {abs/2407.10040},
pages = {null},
doi = {10.48550/arXiv.2407.10040},
arxivid = {2407.10040},
}

@article{faed60347348c362e11c3158a43c3bec851afec0,
title = {Prover Agent: An Agent-based Framework for Formal Mathematical Proofs},
year = {2025},
url = {https://www.semanticscholar.org/paper/faed60347348c362e11c3158a43c3bec851afec0},
abstract = {We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas. These auxiliary lemmas are not limited to subgoals in the formal proof but can also include special cases or potentially useful facts derived from the assumptions, which help in discovering a viable proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems. Our code is publicly available at: https://github.com/kAIto47802/Prover-Agent.},
author = {Kaito Baba and Chaoran Liu and Shuhei Kurita and Akiyoshi Sannai},
journal = {ArXiv},
volume = {abs/2506.19923},
pages = {null},
doi = {10.48550/arXiv.2506.19923},
arxivid = {2506.19923},
}

@article{6cdeaa679833391aefa631cfe2345e2c177a43c5,
title = {Lean-auto: An Interface between Lean 4 and Automated Theorem Provers},
year = {2025},
url = {https://www.semanticscholar.org/paper/6cdeaa679833391aefa631cfe2345e2c177a43c5},
abstract = {Proof automation is crucial to large-scale formal mathematics and software/hardware verification projects in ITPs. Sophisticated tools called hammers have been developed to provide general-purpose proof automation in ITPs such as Coq and Isabelle, leveraging the power of ATPs. An important component of a hammer is the translation algorithm from the ITP's logical system to the ATP's logical system. In this paper, we propose a novel translation algorithm for ITPs based on dependent type theory. The algorithm is implemented in Lean 4 under the name Lean-auto. When combined with ATPs, Lean-auto provides general-purpose, ATP-based proof automation in Lean 4 for the first time. Soundness of the main translation procedure is guaranteed, and experimental results suggest that our algorithm is sufficiently complete to automate the proof of many problems that arise in practical uses of Lean 4. We also find that Lean-auto solves more problems than existing tools on Lean 4's math library Mathlib4.},
author = {Yicheng Qian and Joshua Clune and Clark Barrett and Jeremy Avigad},
doi = {10.48550/arXiv.2505.14929},
arxivid = {2505.14929},
}

@article{72922530fcfa7c1218b2e5a17851f7a36ee05053,
title = {BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/72922530fcfa7c1218b2e5a17851f7a36ee05053},
abstract = {Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating the underlying large proof search spaces. While the existing approaches primarily rely on value functions and/or Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Tree Search (BFS) remains underexplored. In this paper, we investigate whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present BFS-Prover, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. BFS-Prover achieves a state-of-the-art score of $72.95\%$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. To facilitate further research and development in this area, we have open-sourced our model at https://huggingface.co/ByteDance-Seed/BFS-Prover-V1-7B.},
author = {Ran Xin and Chenguang Xi and Jie Yang and Feng Chen and Hang Wu and Xia Xiao and Yifan Sun and Shen Zheng and Kai Shen},
doi = {10.48550/arXiv.2502.03438},
arxivid = {2502.03438},
}

@article{fac63247eebc933c19c8de2809ec68d9c957e6e3,
title = {miniCodeProps: a Minimal Benchmark for Proving Code Properties},
year = {2024},
url = {https://www.semanticscholar.org/paper/fac63247eebc933c19c8de2809ec68d9c957e6e3},
abstract = {AI agents have shown initial promise in automating mathematical theorem proving in proof assistants such as Lean. The same proof assistants can be used to verify the correctness of code by pairing code with specifications and proofs that the specifications hold. Automating the writing of code, specifications, and proofs could lower the cost of verification, or, ambitiously, enable an AI agent to output safe, provably correct code. However, it remains unclear whether current neural theorem provers can automatically verify even relatively simple programs. We present miniCodeProps, a benchmark of 201 program specifications in the Lean proof assistant, aimed at the subproblem of automatically generating a proof for a provided program and specification. miniCodeProps contains specifications about simple, self-contained programs (e.g., lists, natural numbers, binary trees) with varied proof difficulty. Despite its simplicity, miniCodeProps is sufficient to break current LLM-based provers, with state-of-the-art methods showing promise on the easy properties in miniCodeProps, yet failing to prove nearly all of the medium and hard properties. We publicly release miniCodeProps as a benchmark for furthering automated theorem proving in the context of formally verified code.},
author = {Evan Lohn and S. Welleck},
journal = {ArXiv},
volume = {abs/2406.11915},
pages = {null},
doi = {10.48550/arXiv.2406.11915},
arxivid = {2406.11915},
}

@article{43dceda9eb1b15872a939ae34a1c3893b062cfe5,
title = {HybridProver: Augmenting Theorem Proving with LLM-Driven Proof Synthesis and Refinement},
year = {2025},
url = {https://www.semanticscholar.org/paper/43dceda9eb1b15872a939ae34a1c3893b062cfe5},
abstract = {Formal methods is pivotal for verifying the reliability of critical systems through rigorous mathematical proofs. However, its adoption is hindered by labor-intensive manual proofs and the expertise required to use theorem provers. Recent advancements in large language models (LLMs) offer new opportunities for automated theorem proving. Two promising approaches are generating tactics step by step and generating a whole proof directly with an LLM. However, existing work makes no attempt to combine the two approaches. In this work, we introduce HybridProver, a dual-model proof synthesis framework that combines tactic-based generation and whole-proof synthesis to harness the benefits of both approaches. HybridProver generates whole proof candidates for evaluation directly, then extracts proof sketches from those candidates. It then uses a tactic-based generation model that integrates automated tools to complete the sketches via stepwise refinement. We implement HybridProver for the Isabelle theorem prover and fine-tune LLMs on our optimized Isabelle datasets. Evaluation on the miniF2F dataset illustrates HybridProver's effectiveness. We achieve a 59.4% success rate on miniF2F, where the previous SOTA is 56.1%. Our ablation studies show that this SOTA result is attributable to combining whole-proof and tactic-based generation. Additionally, we show how the dataset quality, training parameters, and sampling diversity affect the final result during automated theorem proving with LLMs. All of our code, datasets, and LLMs are open source.},
author = {Jilin Hu and Jianyu Zhang and Yongwang Zhao and Talia Ringer},
journal = {ArXiv},
volume = {abs/2505.15740},
pages = {null},
doi = {10.48550/arXiv.2505.15740},
arxivid = {2505.15740},
}

@article{1ff3d0a6a97aff7717409307bc730b5bc88c8f65,
title = {Leanabell-Prover: Posttraining Scaling in Formal Reasoning},
year = {2025},
url = {https://www.semanticscholar.org/paper/1ff3d0a6a97aff7717409307bc730b5bc88c8f65},
abstract = {Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.},
author = {Jingyuan Zhang and Qi Wang and Xingguang Ji and Yahui Liu and Yang Yue and Fuzheng Zhang and Di Zhang and Guorui Zhou and Kun Gai},
journal = {ArXiv},
volume = {abs/2504.06122},
pages = {null},
doi = {10.48550/arXiv.2504.06122},
arxivid = {2504.06122},
}

@article{c98038be92c3d8bf0e2150f702830373ba708586,
title = {Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/c98038be92c3d8bf0e2150f702830373ba708586},
abstract = {LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.},
author = {Luoxin Chen and Jinming Gu and Liankai Huang and Wenhao Huang and Zhicheng Jiang and Allan Jie and Xiaoran Jin and Xing Jin and Chenggang Li and Kaijing Ma and Cheng Ren and Jiawei Shen and Wenlei Shi and Tong Sun and He Sun and Jiahui Wang and Siran Wang and Zhihong Wang and Chenrui Wei and Shufa Wei and Yong-Xu Wu and Yuchen Wu and Yihang Xia and Hua Xin and Fan Yang and Huaiyuan Ying and Hongyi Yuan and Zheng Yuan and Tianyang Zhan and Chi Zhang and Yue Zhang and Ge-Hang Zhang and Tianyun Zhao and Jianqiu Zhao and Yichi Zhou and Thomas (Hanwen) Zhu},
journal = {ArXiv},
volume = {abs/2507.23726},
pages = {null},
doi = {10.48550/arXiv.2507.23726},
arxivid = {2507.23726},
}

@article{0de4d81b8318633d065694d1816d8cf5a3f7ba95,
title = {Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis},
year = {2025},
url = {https://www.semanticscholar.org/paper/0de4d81b8318633d065694d1816d8cf5a3f7ba95},
abstract = {The synergy between deep learning models and traditional automation tools, such as built-in tactics of the proof assistant and off-the-shelf automated theorem provers, plays a crucial role in developing robust and efficient neural theorem provers(NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when explicitly invoked by the model or at a single granularity level, failing to fully exploit their power. To solve this issue, we propose ProofAug, a procedure that equips LLMs with automation methods at various granularities through fine-grained structure analysis of model-generated proof proposals. ProofAug also serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version) with 2100 queries to the model per problem (In contrast, the previous SOTA in Isabelle, Subgoal-XL, only achieves 56.1% using 16384 queries per problem). We also implement a Lean 4 version of ProofAug that can improve the pass@1 performance of Kimina-Prover-Preview-Distill-1.5B from 44.3% to 50.4% on miniF2F-test. Our code is available at https://github.com/haoxiongliu/ProofAug.},
author = {Haoxiong Liu and Jiacheng Sun and Zhenguo Li and Andrew C Yao},
journal = {ArXiv},
volume = {abs/2501.18310},
pages = {null},
doi = {10.48550/arXiv.2501.18310},
arxivid = {2501.18310},
}

@article{a7ab47f93748a0d839bb9bd5be6376384f8ff34b,
title = {StepFun-Prover Preview: Let's Think and Verify Step by Step},
year = {2025},
url = {https://www.semanticscholar.org/paper/a7ab47f93748a0d839bb9bd5be6376384f8ff34b},
abstract = {We present StepFun-Prover Preview, a large language model designed for formal theorem proving through tool-integrated reasoning. Using a reinforcement learning pipeline that incorporates tool-based interactions, StepFun-Prover can achieve strong performance in generating Lean 4 proofs with minimal sampling. Our approach enables the model to emulate human-like problem-solving strategies by iteratively refining proofs based on real-time environment feedback. On the miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of $70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end training framework for developing tool-integrated reasoning models, offering a promising direction for automated theorem proving and Math AI assistant.},
author = {Shijie Shang and Ruosi Wan and Yue Peng and Yutong Wu and Xiong-hui Chen and Jie Yang and Xiangyu Zhang},
journal = {ArXiv},
volume = {abs/2507.20199},
pages = {null},
doi = {10.48550/arXiv.2507.20199},
arxivid = {2507.20199},
}

@article{ab8f08bbe8b278a20798425d261897d1180c3a9c,
title = {Premise Selection for Lean 4},
year = {null},
url = {https://www.semanticscholar.org/paper/ab8f08bbe8b278a20798425d261897d1180c3a9c},
abstract = {S2 TL;DR: Performance of the MaSh NB, MaSh k -NN, and MeSh algorithms in Lean 4 is comparable to their Isabelle counterparts, while the MePo algorithm does not achieve comparable performance, however, because of differences between the experiment and its Isabelle counterpart.},
author = {Jasmin Blanchette and Jannis Limperg and Anne Baanen},
}

@article{f8b5ee53c3410f20049e7def47bd52403fa388e3,
title = {LEGO-Prover: Neural Theorem Proving with Growing Libraries},
year = {2023},
url = {https://www.semanticscholar.org/paper/f8b5ee53c3410f20049e7def47bd52403fa388e3},
abstract = {Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%). During the proving process, LEGO-Prover also manages to generate over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We also release our code and all the generated skills.},
author = {Huajian Xin and Haiming Wang and Chuanyang Zheng and Lin Li and Zhengying Liu and Qingxing Cao and Yinya Huang and Jing Xiong and Han Shi and Enze Xie and Jian Yin and Zhenguo Li and Xiaodan Liang and Heng Liao},
journal = {ArXiv},
volume = {abs/2310.00656},
pages = {null},
doi = {10.48550/arXiv.2310.00656},
arxivid = {2310.00656},
}

@article{4b94c0cdc52a0cb81d4400109a929d5cf97a3c7a,
title = {REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning},
year = {2025},
url = {https://www.semanticscholar.org/paper/4b94c0cdc52a0cb81d4400109a929d5cf97a3c7a},
abstract = {Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).},
author = {Ziju Shen and N. Huang and Fanyi Yang and Yutong Wang and Guoxiong Gao and Tianyi Xu and Jiedong Jiang and Wanyi He and Pu Yang and Mengzhou Sun and Haocheng Ju and Peihao Wu and Bryan Dai and Bin Dong},
journal = {ArXiv},
volume = {abs/2505.20613},
pages = {null},
doi = {10.48550/arXiv.2505.20613},
arxivid = {2505.20613},
}

@article{27926dae8c5bcf4d67c0bafd7ddd4136fc46b295,
title = {MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation},
year = {2025},
url = {https://www.semanticscholar.org/paper/27926dae8c5bcf4d67c0bafd7ddd4136fc46b295},
abstract = {Automated Theorem Proving (ATP) in formal languages remains a formidable challenge in AI, demanding rigorous logical deduction and navigating vast search spaces. While large language models (LLMs) have shown promising performance, existing stepwise provers often suffer from biased search guidance, leading to inefficiencies and suboptimal proof strategies. This paper introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise ATP system designed to overcome these limitations. MPS-Prover incorporates two key innovations: a highly effective post-training data curation strategy that prunes approximately 40% of redundant training data without sacrificing performance, and a multi-perspective tree search mechanism. This search integrates a learned critic model with strategically designed heuristic rules to diversify tactic selection, prevent getting trapped in unproductive states, and enhance search robustness. Extensive evaluations demonstrate that MPS-Prover achieves state-of-the-art performance on multiple challenging benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter models. Furthermore, our analyses reveal that MPS-Prover generates significantly shorter and more diverse proofs compared to existing stepwise and whole-proof methods, highlighting its efficiency and efficacy. Our work advances the capabilities of LLM-based formal reasoning and offers a robust framework and a comprehensive analysis for developing more powerful theorem provers.},
author = {Zhenwen Liang and Linfeng Song and Yang Li and Tao Yang and Feng Zhang and Haitao Mi and Dong Yu},
journal = {ArXiv},
volume = {abs/2505.10962},
pages = {null},
doi = {10.48550/arXiv.2505.10962},
arxivid = {2505.10962},
}

@article{39be23d432ab9c45243fed3dd0bee47b530d8a92,
title = {Aristotle: IMO-level Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/39be23d432ab9c45243fed3dd0bee47b530d8a92},
abstract = {We introduce Aristotle, an AI system that combines formal verification with informal reasoning, achieving gold-medal-equivalent performance on the 2025 International Mathematical Olympiad problems. Aristotle integrates three main components: a Lean proof search system, an informal reasoning system that generates and formalizes lemmas, and a dedicated geometry solver. Our system demonstrates state-of-the-art performance with favorable scaling properties for automated theorem proving.},
author = {Tudor Achim and Alex Best and Alberto Bietti and Kevin Der and Mathis F'ed'erico and Sergei Gukov and Daniel Halpern-Leistner and Kirsten Henningsgard and Yury Kudryashov and Alexander Meiburg and Martin Michelsen and Riley Patterson and Eric Rodriguez and Laura Scharff and Vikram Shanker and Vladmir Sicca and Hari Sowrirajan and Aidan Swope and Matyas Tamas and Vlad Tenev and Jonathan Thomm and Harold Williams and Lawrence Wu},
journal = {ArXiv},
volume = {abs/2510.01346},
pages = {null},
doi = {10.48550/arXiv.2510.01346},
arxivid = {2510.01346},
}

@article{cd716e9a5edf6394f12aad016da864646af6d676,
title = {Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically},
year = {2024},
url = {https://www.semanticscholar.org/paper/cd716e9a5edf6394f12aad016da864646af6d676},
abstract = {Mathematical theorem proving is an important testbed for large language models' deep and abstract reasoning capability. This paper focuses on improving LLMs' ability to write proofs in formal languages that permit automated proof verification/evaluation. Most previous results provide human-written lemmas to the theorem prover, which is an arguably oversimplified setting that does not sufficiently test the provers' planning and decomposition capabilities. Instead, we work in a more natural setup where the lemmas that are directly relevant to the theorem are not given to the theorem prover at test time. We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas. Our reward mechanism is inspired by how mathematicians train themselves: even if a theorem is too challenging to be proved by the current model, a positive reward is still given to the model for any correct and novel lemmas that are proposed and proved in this process. During training, our model proposes and proves lemmas that are not in the training dataset. In fact, these newly-proposed correct lemmas consist of 37.7% of the training replay buffer when we train on the dataset extracted from Archive of Formal Proofs (AFP). The model trained by our RL algorithm outperforms that trained by supervised finetuning, improving the pass rate from 40.8% to 45.5% on AFP test set, and from 36.5% to 39.5% on an out-of-distribution test set.},
author = {Kefan Dong and Arvind V. Mahankali and Tengyu Ma},
journal = {ArXiv},
volume = {abs/2411.01829},
pages = {null},
doi = {10.48550/arXiv.2411.01829},
arxivid = {2411.01829},
}

@article{c116dd5aac94385dcd8626e4f4991d99e9ce5451,
title = {Solving Formal Math Problems by Decomposition and Iterative Reflection},
year = {2025},
url = {https://www.semanticscholar.org/paper/c116dd5aac94385dcd8626e4f4991d99e9ce5451},
abstract = {General-purpose Large Language Models (LLMs) have achieved remarkable success in intelligence, performing comparably to human experts on complex reasoning tasks such as coding and mathematical reasoning. However, generating formal proofs in specialized languages like Lean 4 remains a significant challenge for these models, limiting their application in complex theorem proving and automated verification. Current approaches typically require specializing models through fine-tuning on dedicated formal corpora, incurring high costs for data collection and training. In this work, we introduce \textbf{Delta Prover}, an agent-based framework that orchestrates the interaction between a general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages the reflection and reasoning capabilities of general-purpose LLMs to interactively construct formal proofs in Lean 4, circumventing the need for model specialization. At its core, the agent integrates two novel, interdependent components: an algorithmic framework for reflective decomposition and iterative proof repair, and a custom Domain-Specific Language (DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test benchmark, surpassing all existing approaches, including those requiring model specialization.} Furthermore, Delta Prover exhibits a significantly stronger test-time scaling law compared to standard Best-of-N proof strategies. Crucially, our findings demonstrate that general-purpose LLMs, when guided by an effective agentic structure, possess substantial untapped theorem-proving capabilities. This presents a computationally efficient alternative to specialized models for robust automated reasoning in formal environments.},
author = {Yichi Zhou and Jianqiu Zhao and Yongxin Zhang and Bohan Wang and Siran Wang and Luoxin Chen and Jiahui Wang and Haowei Chen and Allan Jie and Xinbo Zhang and Haocheng Wang and L. Trung and Rong Ye and Phan Nhat Hoang and Huishuai Zhang and Peng Sun and Hang Li},
journal = {ArXiv},
volume = {abs/2507.15225},
pages = {null},
doi = {10.48550/arXiv.2507.15225},
arxivid = {2507.15225},
}

@article{043aef64488f36fad56b71fb852c5cb027e87245,
title = {Local Look-Ahead Guidance via Verifier-in-the-Loop for Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/043aef64488f36fad56b71fb852c5cb027e87245},
abstract = {The most promising recent methods for AI reasoning require applying variants of reinforcement learning (RL) either on rolled out trajectories from the LLMs, even for the step-wise rewards, or large quantities of human-annotated trajectory data. The reliance on the rolled-out trajectory renders the compute cost and time prohibitively high. In particular, the correctness of a reasoning trajectory can typically only be judged at its completion, leading to sparse rewards in RL or requiring expensive synthetic data generation in expert iteration-like methods. In this work, we focus on the Automatic Theorem Proving (ATP) task and propose a novel verifier-in-the-loop design, which, unlike existing approaches that leverage feedback on the entire reasoning trajectory, employs an automated verifier to give intermediate feedback at each step of the reasoning process. Using Lean as the verifier, we empirically show that the step-by-step local verification produces a global improvement in the model's reasoning accuracy and efficiency.},
author = {Sara Rajaee and Kumar Pratik and Gabriele Cesa and Arash Behboodi},
doi = {10.48550/arXiv.2503.09730},
arxivid = {2503.09730},
}

@article{7de36d6b14aadc8cdb6ad1340b9ca64b15375bca,
title = {Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
year = {2022},
url = {https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
abstract = {The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.},
author = {Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timoth√©e Lacroix and Yuhuai Wu and Guillaume Lample},
journal = {ArXiv},
volume = {abs/2210.12283},
pages = {null},
doi = {10.48550/arXiv.2210.12283},
arxivid = {2210.12283},
}

@article{29d2bf6d4b0ce5cdd2cf0ad3103597ba5681f29f,
title = {Proving Theorems Recursively},
year = {2024},
url = {https://www.semanticscholar.org/paper/29d2bf6d4b0ce5cdd2cf0ad3103597ba5681f29f},
abstract = {Recent advances in automated theorem proving leverages language models to explore expanded search spaces by step-by-step proof generation. However, such approaches are usually based on short-sighted heuristics (e.g., log probability or value function scores) that potentially lead to suboptimal or even distracting subgoals, preventing us from finding longer proofs. To address this challenge, we propose POETRY (PrOvE Theorems RecursivelY), which proves theorems in a recursive, level-by-level manner in the Isabelle theorem prover. Unlike previous step-by-step methods, POETRY searches for a verifiable sketch of the proof at each level and focuses on solving the current level's theorem or conjecture. Detailed proofs of intermediate conjectures within the sketch are temporarily replaced by a placeholder tactic called sorry, deferring their proofs to subsequent levels. This approach allows the theorem to be tackled incrementally by outlining the overall theorem at the first level and then solving the intermediate conjectures at deeper levels. Experiments are conducted on the miniF2F and PISA datasets and significant performance gains are observed in our POETRY approach over state-of-the-art methods. POETRY on miniF2F achieves an average proving success rate improvement of 5.1%. Moreover, we observe a substantial increase in the maximum proof length found by POETRY, from 10 to 26.},
author = {Haiming Wang and Huajian Xin and Zhengying Liu and Wenda Li and Yinya Huang and Jianqiao Lu and Zhicheng YANG and Jing Tang and Jian Yin and Zhenguo Li and Xiaodan Liang},
journal = {ArXiv},
volume = {abs/2405.14414},
pages = {null},
doi = {10.48550/arXiv.2405.14414},
arxivid = {2405.14414},
}

@article{e1a642026fb46a8b8a868862bcf0728e8d215d7e,
title = {DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search},
year = {2024},
url = {https://www.semanticscholar.org/paper/e1a642026fb46a8b8a868862bcf0728e8d215d7e},
abstract = {We introduce DeepSeek-Prover-V1.5, an open-source language model designed for theorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both training and inference processes. Pre-trained on DeepSeekMath-Base with specialization in formal mathematical languages, the model undergoes supervised fine-tuning using an enhanced formal theorem proving dataset derived from DeepSeek-Prover-V1. Further refinement is achieved through reinforcement learning from proof assistant feedback (RLPAF). Beyond the single-pass whole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5 demonstrates significant improvements over DeepSeek-Prover-V1, achieving new state-of-the-art results on the test set of the high school level miniF2F benchmark ($63.5\%$) and the undergraduate level ProofNet benchmark ($25.3\%$).},
author = {Huajian Xin and Z. Ren and Jun-Mei Song and Zhihong Shao and Wanjia Zhao and Haocheng Wang and Bo Liu (Benjamin Liu) and Liyue Zhang and Xuan Lu and Qiushi Du and Wenjun Gao and Qihao Zhu and Dejian Yang and Zhibin Gou and Z. F. Wu and Fuli Luo and C. Ruan},
journal = {ArXiv},
volume = {abs/2408.08152},
pages = {null},
doi = {10.48550/arXiv.2408.08152},
arxivid = {2408.08152},
}

@article{1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3,
title = {Lean Workbook: A large-scale Lean problem set formalized from natural language math problems},
year = {2024},
url = {https://www.semanticscholar.org/paper/1977056dc5b7cbbc26b2210a6d6d1a1e3ce2dad3},
abstract = {Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.},
author = {Huaiyuan Ying and Zijian Wu and Yihan Geng and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2406.03847},
pages = {null},
doi = {10.48550/arXiv.2406.03847},
arxivid = {2406.03847},
}

@article{94cc81740acd84111386d4cbb4562e6e65f7f38c,
title = {Logic and Proof},
year = {1999},
url = {https://www.semanticscholar.org/paper/94cc81740acd84111386d4cbb4562e6e65f7f38c},
abstract = {null},
author = {Lawrence Charles Paulson},
}

@article{058ce15272a76a1c6376e7987b28644067f1ef92,
title = {SubgoalXL: Subgoal-based Expert Learning for Theorem Proving},
year = {2024},
url = {https://www.semanticscholar.org/paper/058ce15272a76a1c6376e7987b28644067f1ef92},
abstract = {Formal theorem proving, a field at the intersection of mathematics and computer science, has seen renewed interest with advancements in large language models (LLMs). This paper introduces SubgoalXL, a novel approach that synergizes subgoal-based proofs with expert learning to enhance LLMs' capabilities in formal theorem proving within the Isabelle environment. SubgoalXL addresses two critical challenges: the scarcity of specialized mathematics and theorem-proving data, and the need for improved multi-step reasoning abilities in LLMs. By optimizing data efficiency and employing subgoal-level supervision, SubgoalXL extracts richer information from limited human-generated proofs. The framework integrates subgoal-oriented proof strategies with an expert learning system, iteratively refining formal statement, proof, and subgoal generators. Leveraging the Isabelle environment's advantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art performance of 56.1\% in Isabelle on the standard miniF2F dataset, marking an absolute improvement of 4.9\%. Notably, SubgoalXL successfully solves 41 AMC12, 9 AIME, and 3 IMO problems from miniF2F. These results underscore the effectiveness of maximizing limited data utility and employing targeted guidance for complex reasoning in formal theorem proving, contributing to the ongoing advancement of AI reasoning capabilities. The implementation is available at \url{https://github.com/zhaoxlpku/SubgoalXL}.},
author = {Xueliang Zhao and Lin Zheng and Haige Bo and Changran Hu and Urmish Thakker and Lingpeng Kong},
journal = {ArXiv},
volume = {abs/2408.11172},
pages = {null},
doi = {10.48550/arXiv.2408.11172},
arxivid = {2408.11172},
}

@article{65b4b25272c50dc376f5c018338931bfd349e532,
title = {HyperTree Proof Search for Neural Theorem Proving},
year = {2022},
url = {https://www.semanticscholar.org/paper/65b4b25272c50dc376f5c018338931bfd349e532},
abstract = {We propose an online training procedure for a transformer-based automated theorem prover. Our approach leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero. Our model learns from previous proof searches through online training, allowing it to generalize to domains far from the training distribution. We report detailed ablations of our pipeline's main components by studying performance on three environments of increasing complexity. In particular, we show that with HTPS alone, a model trained on annotated proofs manages to prove 65.4% of a held-out set of Metamath theorems, significantly outperforming the previous state of the art of 56.5% by GPT-f. Online training on these unproved theorems increases accuracy to 82.6%. With a similar computational budget, we improve the state of the art on the Lean-based miniF2F-curriculum dataset from 31% to 42% proving accuracy.},
author = {Guillaume Lample and M. Lachaux and Thibaut Lavril and Xavier Martinet and Amaury Hayat and Gabriel Ebner and Aur'elien Rodriguez and Timoth√©e Lacroix},
journal = {ArXiv},
volume = {abs/2205.11491},
pages = {null},
doi = {10.48550/arXiv.2205.11491},
arxivid = {2205.11491},
}

@article{eda90d3d4f8515b778e8255399c1bd950009c1a0,
title = {ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data},
year = {2025},
url = {https://www.semanticscholar.org/paper/eda90d3d4f8515b778e8255399c1bd950009c1a0},
abstract = {Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of the student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. Running the proposed ATLAS framework for 10 iterations, we construct an undergraduate-level dataset of 117k theorem statements and develop the ATLAS Translator by fine-tuning Llama3.1-8B-Instruct with LoRA. This model establishes a new state of the art, demonstrating statistically significant improvements over both the Herald Translator and the Kimina-Autoformalizer across all benchmarks (p<0.05, two-sided t-test). Furthermore, we demonstrate that the full-parameter fine-tuning of a stronger base model on the ATLAS dataset leads to superior performance. The datasets, model, and code are available at https://github.com/XiaoyangLiu-sjtu/ATLAS.},
author = {Xiaoyang Liu and Kangjie Bao and Jiashuo Zhang and Yunqi Liu and Yu Chen and Yuntian Liu and Yang Jiao and Tao Luo},
journal = {ArXiv},
volume = {abs/2502.05567},
pages = {null},
doi = {10.48550/arXiv.2502.05567},
arxivid = {2502.05567},
}

@article{3393fa3da068bbd04af6e7716ec3f60e688af1b0,
title = {Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models},
year = {2025},
url = {https://www.semanticscholar.org/paper/3393fa3da068bbd04af6e7716ec3f60e688af1b0},
abstract = {Recent advancements, such as DeepSeek-Prover-V2-671B and Kimina-Prover-Preview-72B, demonstrate a prevailing trend in leveraging reinforcement learning (RL)-based large-scale training for automated theorem proving. Surprisingly, we discover that even without any training, careful neuro-symbolic coordination of existing off-the-shelf reasoning models and tactic step provers can achieve comparable performance. This paper introduces \textbf{DSP+}, an improved version of the Draft, Sketch, and Prove framework, featuring a \emph{fine-grained and integrated} neuro-symbolic enhancement for each phase: (1) In the draft phase, we prompt reasoning models to generate concise natural-language subgoals to benefit the sketch phase, removing thinking tokens and references to human-written proofs; (2) In the sketch phase, subgoals are autoformalized with hypotheses to benefit the proving phase, and sketch lines containing syntactic errors are masked according to predefined rules; (3) In the proving phase, we tightly integrate symbolic search methods like Aesop with step provers to establish proofs for the sketch subgoals. Experimental results show that, without any additional model training or fine-tuning, DSP+ solves 80.7\%, 32.8\%, and 24 out of 644 problems from miniF2F, ProofNet, and PutnamBench, respectively, while requiring fewer budgets compared to state-of-the-arts. DSP+ proves \texttt{imo\_2019\_p1}, an IMO problem in miniF2F that is not solved by any prior work. Additionally, DSP+ generates proof patterns comprehensible by human experts, facilitating the identification of formalization errors; For example, eight wrongly formalized statements in miniF2F are discovered. Our results highlight the potential of classical reasoning patterns besides the RL-based training. All components will be open-sourced.},
author = {Chenrui Cao and Liangcheng Song and Zenan Li and Xinyi Le and Xian Zhang and Hui Xue and Fan Yang},
journal = {ArXiv},
volume = {abs/2506.11487},
pages = {null},
doi = {10.48550/arXiv.2506.11487},
arxivid = {2506.11487},
}

@article{6ef5a9e475c3596cb42a4e320b9f0a210e06c0f4,
title = {CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics},
year = {2025},
url = {https://www.semanticscholar.org/paper/6ef5a9e475c3596cb42a4e320b9f0a210e06c0f4},
abstract = {Neurosymbolic approaches integrating large language models with formal reasoning have recently achieved human-level performance on mathematics competition problems in algebra, geometry and number theory. In comparison, combinatorics remains a challenging domain, characterized by a lack of appropriate benchmarks and theorem libraries. To address this gap, we introduce CombiBench, a comprehensive benchmark comprising 100 combinatorial problems, each formalized in Lean~4 and paired with its corresponding informal statement. The problem set covers a wide spectrum of difficulty levels, ranging from middle school to IMO and university level, and span over ten combinatorial topics. CombiBench is suitable for testing IMO solving capabilities since it includes all IMO combinatorial problems since 2000 (except IMO 2004 P3 as its statement contain an images). Furthermore, we provide a comprehensive and standardized evaluation framework, dubbed Fine-Eval (for $\textbf{F}$ill-in-the-blank $\textbf{in}$ L$\textbf{e}$an Evaluation), for formal mathematics. It accommodates not only proof-based problems but also, for the first time, the evaluation of fill-in-the-blank questions. Using Fine-Eval as the evaluation method and Kimina Lean Server as the backend, we benchmark several LLMs on CombiBench and observe that their capabilities for formally solving combinatorial problems remain limited. Among all models tested (none of which has been trained for this particular task), Kimina-Prover attains the best results, solving 7 problems (out of 100) under both ``with solution'' and ``without solution'' scenarios. We open source the benchmark dataset alongside with the code of the proposed evaluation method at https://github.com/MoonshotAI/CombiBench/.},
author = {Junqi Liu and Xiaohan Lin and Jonas Bayer and Yael Dillies and Weijie Jiang and Xiaodan Liang and Roman Soletskyi and Haiming Wang and Yunzhou Xie and Beibei Xiong and Zheng-Sheng Yang and Jujian Zhang and Lihong Zhi and Jia Li and Zhengying Liu},
journal = {ArXiv},
volume = {abs/2505.03171},
pages = {null},
doi = {10.48550/arXiv.2505.03171},
arxivid = {2505.03171},
}

@article{85c4ada888e7c363e671a6398d8a7b99cf890317,
title = {LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover},
year = {2024},
url = {https://www.semanticscholar.org/paper/85c4ada888e7c363e671a6398d8a7b99cf890317},
abstract = {Recently, large language models have presented promising results in aiding formal mathematical reasoning. However, their performance is restricted due to the scarcity of formal theorem-proving data, which requires additional effort to be extracted from raw formal language corpora. Meanwhile, a significant amount of human-written formal language corpora remains underutilized. To address this issue, we propose LEAN-GitHub, a dataset consisting of large-scale formal data extracted from almost all Lean 4 repositories on GitHub. After fine-tuning InternLM-math-plus on this dataset, our model achieved accuracies of 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F test, surpassing state-of-the-art method at 52%. And it also achieves state-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting different fields/levels of math. These results demonstrate that our proposed dataset is beneficial for formal reasoning on a wide range of math topics. We open-source our model at https://GitHub. com/InternLM/InternLM-Math and our data at https://huggingface.co/ datasets/InternLM/Lean-GitHub},
author = {Zijian Wu and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2407.17227},
pages = {null},
doi = {10.48550/arXiv.2407.17227},
arxivid = {2407.17227},
}

@article{52bbfafbc63866d95015027ae17595fc3396edb0,
title = {LLM-based Automated Theorem Proving Hinges on Scalable Synthetic Data Generation},
year = {2025},
url = {https://www.semanticscholar.org/paper/52bbfafbc63866d95015027ae17595fc3396edb0},
abstract = {Recent advancements in large language models (LLMs) have sparked considerable interest in automated theorem proving and a prominent line of research integrates stepwise LLM-based provers into tree search. In this paper, we introduce a novel proof-state exploration approach for training data synthesis, designed to produce diverse tactics across a wide range of intermediate proof states, thereby facilitating effective one-shot fine-tuning of LLM as the policy model. We also propose an adaptive beam size strategy, which effectively takes advantage of our data synthesis method and achieves a trade-off between exploration and exploitation during tree search. Evaluations on the MiniF2F and ProofNet benchmarks demonstrate that our method outperforms strong baselines under the stringent Pass@1 metric, attaining an average pass rate of $60.74\%$ on MiniF2F and $21.18\%$ on ProofNet. These results underscore the impact of large-scale synthetic data in advancing automated theorem proving.},
author = {Junyu Lai and Jiakun Zhang and Shuo Xu and Taolue Chen and Zihang Wang and Yao Yang and Jiarui Zhang and Chun Cao and Jingwei Xu},
journal = {ArXiv},
volume = {abs/2505.12031},
pages = {null},
doi = {10.48550/arXiv.2505.12031},
arxivid = {2505.12031},
}

@article{10f221d9c0ebaa8c5a0ea53fdec404bbda3c421c,
title = {ProofCompass: Enhancing Specialized Provers with LLM Guidance},
year = {2025},
url = {https://www.semanticscholar.org/paper/10f221d9c0ebaa8c5a0ea53fdec404bbda3c421c},
abstract = {Language models have become increasingly powerful tools for formal mathematical reasoning. However, most existing approaches rely exclusively on either large general-purpose models or smaller specialized models, each with distinct limitations, while training specialized large models still requires significant computational resources. This paper introduces ProofCompass, a novel hybrid methodology that achieves remarkable computational efficiency by strategically guiding existing specialized prover methods, such as DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without requiring additional model training. The LLM provides natural language proof strategies and analyzes failed attempts to select intermediate lemmas, enabling effective problem decomposition. On the miniF2F benchmark, ProofCompass demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\% \rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$). Our synergistic approach paves the way for simultaneously improving computational efficiency and accuracy in formal theorem proving.},
author = {Nicolas Wischermann and C. M. Verdun and Gabriel Poesia and Francesco Noseda},
journal = {ArXiv},
volume = {abs/2507.14335},
pages = {null},
doi = {10.48550/arXiv.2507.14335},
arxivid = {2507.14335},
}

@article{5cf103a9610d3f1bb11ec254281f2e01284819b1,
title = {Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction},
year = {2025},
url = {https://www.semanticscholar.org/paper/5cf103a9610d3f1bb11ec254281f2e01284819b1},
abstract = {We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.},
author = {Yong Lin and Shange Tang and Bohan Lyu and Ziran Yang and Jui-Hui Chung and Haoyu Zhao and Lai Jiang and Yihan Geng and Jiawei Ge and Jingruo Sun and Jiayun Wu and Jiri Gesi and Ximing Lu and David Acuna and Kaiyu Yang and Hongzhou Lin and Yejin Choi and Danqi Chen and Sanjeev Arora and Chi Jin},
journal = {ArXiv},
volume = {abs/2508.03613},
pages = {null},
doi = {10.48550/arXiv.2508.03613},
arxivid = {2508.03613},
}

@article{5b3cd003ae273b5ae9c29259e1db1b67a97cfd4e,
title = {InternLM2.5-StepProver: Advancing Automated Theorem Proving via Expert Iteration on Large-Scale LEAN Problems},
year = {2024},
url = {https://www.semanticscholar.org/paper/5b3cd003ae273b5ae9c29259e1db1b67a97cfd4e},
abstract = {S2 TL;DR: This work proposes to use large-scale LEAN problem datasets Lean-workbook for expert iteration with more than 20,000 CPU days and finds log-linear trends between solved problem amount with proof length and CPU usage.},
author = {Zijian Wu and Suozhi Huang and Zhejian Zhou and Huaiyuan Ying and Jiayu Wang and Dahua Lin and Kai Chen},
journal = {ArXiv},
volume = {abs/2410.15700},
pages = {null},
doi = {10.48550/arXiv.2410.15700},
arxivid = {2410.15700},
}

@article{e4a6ccfb7db7e0f8c55424563e01bf9df768ecb3,
title = {Rango: Adaptive Retrieval-Augmented Proving for Automated Software Verification},
year = {2024},
url = {https://www.semanticscholar.org/paper/e4a6ccfb7db7e0f8c55424563e01bf9df768ecb3},
abstract = {Formal verification using proof assistants, such as Coq, enables the creation of high-quality software. However, the verification process requires significant expertise and manual effort to write proofs. Recent work has explored automating proof synthesis using machine learning and large language models (LLMs). This work has shown that identifying relevant premises, such as lemmas and definitions, can aid synthesis. We present Rango, a fully automated proof synthesis tool for Coq that automatically identifies relevant premises and also similar proofs from the current project and uses them during synthesis. Rango uses retrieval augmentation at every step of the proof to automatically determine which proofs and premises to include in the context of its fine-tuned LLM. In this way, Rango adapts to the project and to the evolving state of the proof. We create a new dataset, CoqStoq, of 2,226 open-source Coq projects and 196,929 theorems from GitHub, which includes both training data and a curated evaluation benchmark of well-maintained projects. On this benchmark, Rango synthesizes proofs for 32.0% of the theorems, which is 29% more theorems than the prior state-of-the-art tool Tactician. Our evaluation also shows that Rango adding relevant proofs to its context leads to a 47% increase in the number of theorems proven.},
author = {Kyle Thompson and Nuno Saavedra and Pedro Carrott and Kevin Fisher and Alex Sanchez-Stern and Yuriy Brun and Jo√£o F. Ferreira and Sorin Lerner and Emily First},
journal = {2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)},
volume = {null},
pages = {347-359},
doi = {10.1109/ICSE55347.2025.00161},
arxivid = {2412.14063},
}

@article{a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68,
title = {Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving},
year = {2025},
url = {https://www.semanticscholar.org/paper/a05d5102ce84d1830e1e0d8b7a6a7918bd9cfb68},
abstract = {We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art (as of April 5 2025) performance in automated formal proof generation for mathematical problems. A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches. First, we train LLMs to convert natural language math problems from the Numina dataset to equivalent formal statements in Lean 4. This process creates the dataset Goedel-Pset-v1, which includes 1.64 million formal statements. Next, we develop a large dataset of formal proofs by training a series of provers. Each new prover can prove many statements that previous ones could not, and these new proofs are added to the training set for the next prover. Finally, we obtain the dataset Goedel-Pset-v1-solved, which contains proofs for over 800K statements from Goedel-Pset-v1. Supervised fine-tuning (SFT) of DeepSeek-Prover-V1.5-Base on Goedel-Pset-v1-solved (i.e., no RL) yields a Goedel-Prover-SFT that achieves a success rate of 57.6% (Pass@32) on miniF2F, surpassing the previous leader DeepSeek-Prover-V1.5-RL (trained using SFT + RL on a proprietary dataset) by 7.6%. On PutnamBench, Goedel-Prover-SFT successfully solves 7 problems (Pass@512), ranking first on the leaderboard. We provide extensive discussion of our training methodology, highlighting the key design choices that contribute to Goedel-Prover's strong performance. Further RL training (including DPO) improves Goedel-Prover-SFT's success rate to over 60% (Pass@32) on miniF2F. To aid future research, we provide extensive discussion of our training methodology and design choices. We also fully open-source our codes, models, and datasets. Additionally, we open-source formal proofs for 29.7K problems in Lean Workbook, nearly doubling the 15.7K solved by prior provers.},
author = {Yong Lin and Shange Tang and Bohan Lyu and Jiayun Wu and Hongzhou Lin and Kaiyu Yang and Jia Li and Mengzhou Xia and Danqi Chen and Sanjeev Arora and Chi Jin},
journal = {ArXiv},
volume = {abs/2502.07640},
pages = {null},
doi = {10.48550/arXiv.2502.07640},
arxivid = {2502.07640},
}
