Karina, thank you so much for joining me today. Thank you so much for having me. All right, you are working on the mathematics, mathematics plus AI and um it's one of my all-time passions, loves of life. So, I'm very very excited to chat about it. Um let's start with the the premise of what you're working on. um self-improving mathematical super intelligence. So as as for for starters, can you explain in plain English what that is? Yeah, 100%. Um so first of all, there's mathematics and there's super intelligence. Um when it comes to mathematics, it's everyone's favorite topic nowadays. Um AI systems just won the IMO gold medal this year. Uh and actually many AI systems um the same time won the medal. Um people are seeing large language models getting better and better at reasoning and reasoning is broadly defined as math and coding. It makes sense to define it this way because in fact math and coding are the two pillars of the digital world and quoting Eric Smith's. Um we have seen large language models have incredible gains over coding in the past like year two years or so. Um on the math side however it's you know sort of less invested in and it's for a reason. Currently large language models train on train of thought reasoning. Uh so for every math problem they will have experts to um describe the train of thought and then train on those data. On the other hand when it comes to coding it's not quite just that they will for example train on executable computer programs. So you can either see a check mark telling you the code has compiled successfully or there is an error message. So because of this difference uh we see um coding being a verifiable domain um has had incredible gains versus for math um the verification comes from problems that have one numerical answer uh instead of a whole proof. So um all the problems that have you know pretty in intricate logical reasoning get reduced to the final output and XM is um working to change that. We want to turn math proofs into computer programs and using similar techniques um in reasoning and in the development of coding we can make really uh significant breakthroughs in the domain of mathematics as well. So that's mathematics. On the topic of super intelligence why we call it super intelligence it's it's not just a hype word. Um we define super intelligence at AI system that can generate new knowledge instead of just answering questions um the user put in. We imagine the system to be your collaborator, co-author, AI scientist, AI mathematician and the important defining characteristic is the ability to generate denovo new knowledge. Currently we will pay experts from um data labelers like Mccor and Touring to write Olympia math problems in the future. So hopefully the AI system can tell you hey these are the math questions that are worthy to pursue. So combining the the two XIM is building toward a system that can um prove conjecture and make discoveries um at an unprecedented scale and speed and therefore we think it's really exciting to be building mathematical super intelligence. That's amazing. uh the premise as as you mentioned is is just incredible and the entire digital world is built upon this. Can you can you talk about the the similarities and differences when it comes to applying AI for coding uh versus u using it for mathematical proofs. So where where can we use common fundamentals and where does it start to diverge where we need a different system to build a proofs that can be verified by other mathematicians? Yeah. So I I say um when it comes to um turning mass proofs into computer programs there is one good news and one bad news. Uh the good news is um like you know in coding you have Python these are programming languages. Um there is a programming language for proofs. It's called lean lan and um lean after many years of development I think since um almost a decade ago it was first developed at Microsoft and um now there are so many groups working on lean lean 4 has become uh widely uh accepted and celebrated in the community and it's also a lot easier to to use as just you know a matter of fact of engineering um um um efficiency and and so therefore um because of a theorem called um Ky Howard correspondence for every mathematical proof once you turn it into the language of lean because of how lean is set up and there's a lot of dependent type theory behind it you can actually run the mathematical proof so that essentially uh turns this into a programming language related problem now the bad news is well there are more than I think a trillion tokens of code on the internet you know that's free for folks to scrape uh there's a data scarcity problem here data scarcity for um link improves um there are at most I think uh 10 million tokens of link code so you're seeing about 100,000 times data gap this data gap is difficult to work with in an era of scaling we want to close this data gap and at Axiom we are trying to have an internet scale data set for lean mass proofs as well yeah lean is is um yeah it's it's it's incredible how much progress has has happened and and we'll touch upon that. Now, when you look at an AI system proving something um and if you have to explain that to somebody who's not from mathematics or coding like what's a what's an analogy or how do you explain how AI proves things? How does it work? Yeah. Yeah. I think so. Are you talking about maybe like the lean part or the general AI proving part? Uh actually, let's do though. Let's start with the general AI proving part like let's say you want to arrive at some mathematical proof like how does it how does it know how to navigate because the number of possibilities you know a combinatorial explosion can happen so how does it prove that and also maybe at implementation how does it happen inside the lean language yeah 100% I think if you look at for example the K12 education system um in middle school and high school a lot of the math problems we encounter are um fill in the blank kind of questions like count to the number of appos uh that satisfy certain property. Now when you move to college um real mathematics um the ability to think in proofbased language is an important milestone for development of mathematical capability. In fact at many universities there is a class called introduction to proofs. uh we have been trained in a way humans have been trained to write two column proofs um most mostly starting point in college uh we will make certain assumptions and then deduce logically based on those assumptions the next step so it's almost reducing the original um problem to a new problem intermediate state an intermediate goal that we would try to prove so that's actually um it's backward and forward conjecturing Right? So, uh it can either be hey I have this problem and if I don't prove this then there's no way I'm going to prove that. So that's backward deduction. forward deduction is given the conditions provided by the problem. Uh what are some immediate things I can say about say the property of this set and so if I can say that all the sets that satisfy the conditions this problem described um have another property then I then get a bit closer to from from my point of view from my point of knowing nothing to the problem statement. So that's forward deduction. So in the sort of backward and forward deduction eventually this gap close um and bingo you have proved the thing that you're asked to prove. So that's how humans work on proofs. Um currently in general informal language natural language AI system we will see um chatbt for example telling you the stepby-step walkthrough of uh how a proof should be. The problem is however there's no way unless you are an expert um to to know uh exactly whether it is true or not true and you also don't pinpoint a step where it starts to become problematic. That's why a lot of um mathematicians don't trust the output of general large language models. On the other hand, when it comes to lean uh it's functions more similar to Python than to natural language English deduction. um we see the program start you know defining defining new objects that show up in the problem statements how like how you would um define um things in in Python and then it will uh continue building on little sub goals just like how a human mathematician would deduce and then eventually it will prove the desired statement and then you will see the complete code. Then you run on the code to see either it compiles a check mark or an error message at say line 37 which you then like look at it oh what is going on and you try to debug. So that's quite interesting because for the first time in human history uh well mathematicians have been working on logical reasoning rigorous logical reasoning using pen and paper for thousands of years. uh now we can digitalize the process of mathematical proofs and bring that closer to programming language that we're familiar with. Right. Um that's amazing. Love love the the chain of thought here. Um you are very accomplished in mathematics and there's so much more to go. So I would love to maybe take a second to just talk about now we'll talk about how machines do it but you as a human um can you just walk us through your first exposure to mathematics the earliest thing you can remember and also today uh how do you look at a problem what do you do like how do you start attacking it how do you break it down how do you how do you build a solution yeah 100%. Um so I I love math like for for um as long as I can remember. Uh when I was eight I would go to uh these uh sort of u mathematics idiate classes and work on really interesting and creative problems that's beyond the school curriculum. Then I kind of went along in the Olympic math system and eventually went to MIT uh where I surround myself with like classmates that are IMO gold medalist in their respective countries and I'm like oh man I don't know math at all but then an interesting sort of transition happened where we are Olympiate math problem solvers to research mathematicians. So there are two kinds of mathematicians broadly. One is really good is good good at problem solving. The other one kind um is really good at theory building. They will connect the dots between different disciplines, formulate new hypothesis and new conjectures and then either invite their collaborators to solve it or try solving it themselves. So the ability to go broadly um read a lot of the literature and then to connect the different dots the theory building uh ability is in is really fundamental to the development of mathematic that's how new fields are being proposed and and discovered and then you also need the problem solvers obviously to try to fill out the gaps of oh wow okay there are 12 conjectures because of the introduction of this one interesting definition then how do I prove these 12 conjectures and then push beyond that. It is a collaborative effort between mathematicians of different skill sets. So at MIT I try to learn how to be a theory builder um and on top of uh having trained to be a problem solver and that is quite interesting because if you look at large language models today they're really good at problem solving but they don't propose new hypothesis and conjectures or they try to but you have no way to verify that. So um that is the question of intuition and taste. How do you know if the chatbot is telling you a definition that is worth investigating further into or something that just you know made up by pattern matching. Now what accent wants to do is we want to through formal reasoning and informal reasoning combined have one system that not only prove but also conjectures and theory build and and that's quite exciting because um for example the ability to introduce new definitions into the knowledgebased library learning is traditionally hard. So we're very excited to be working on these stubborn technical challenges and um on your question about how a mathematician will think about a problem. So we don't just compute the numerical output. There are many intermediate intermediate states that we need to travel to before we arrive at the final answer. And in that process we are doing proofs. um we are trying to prove lemmas one at a time and then do hierarchical planning of bridging these different lemas almost like Lego bricks to form a complicated proof. Now this is a complex multi-step reasoning. Humans do it by either having seen similar things which is pattern matching or by um just using the deduction ability. um what is try to think you know uh by different it's almost like a research like you know what are the five possible ways of moving forward. We also do a lot of sanity checks. For example, we have this um lema that we think could be useful. Then immediately in our brain, we construct either examples to boost our confidence about this lema or counter examples that um tell us that well actually it's just not true and you should go another path and then you can backtrack from there. All these are relevant in developing an AI mathematician. Amazing. with that I would love to dive into the the engine room. So you you founded a company called Axium that you mentioned you raised a big round and congratulations on that. Uh now you mentioned that uh the data is one of the limiting things like for example as compared to the trillions of tokens available for average like text generation uh mathematical proves there just isn't enough data. So when you look at the the raw ingredients your system needs to to think and reason. Um is is data the real bottleneck? Do you have enough compute available? Do you have people? So can you talk about what the bottlenecks are and what you need to to get to the next big breakthrough? For sure. Uh so I think for building an AI mathematician um we need a few things. Um one we need data. uh we need a really large lean data set beyond what is currently available. Um so and then we also need a good lean infrastructure. So for example um if you want to run anything on lean it can be slow and buggy if the infrastructure isn't set up right. Uh we also need a lot of strong engineering execution um for for example building a knowledge base of all the lemmas and proofs that's currently available. So if you think about all the archive papers um how can you sort of you know translate them or auto formalize them or formalize them into the lean digital data set. We believe a citation based knowledge graph is important um and we are doing large scale engineering to to do that. Um so there are like it's a multi- you know multicomponent process. We have um some amount of funding that we think are going to support our uh compute need um and also talent need. We have the strongest team uh can possibly imagine um coming from AI, programming language and mathematics. We have word experts and out of one uh for each of these three fields coming together to build towards this dream and mission. So we feel really really uh happy about that. Um and then it's really about just execution. We we want to be um very fast to sort of uh climb to the frontier um qualify soda of um harder and harder benchmarks. So um yeah, so that's kind of the the setup. Now you're uh when your AI system solves a problem, I mean you have to you give it a problem and it has to navigate and check different paths and build a solution. So how does it check its own work in plain terms? Because you don't want to you don't want it to go on a wrong path too fat too deep before knowing it's wrong. So how do you how does it check? Yeah. So um there there again informal reasoning system and the formal reasoning system. The formal reasoning system side is it's quite simple. Um frankly you you run the proof uh you run the proof and then you see if it compiles. um the the the setup of lean as a programming language allows us to do that and that's just incredible on the informal reasoning side um typically people have tried to use LMS judges so a judge system to critique bot to say okay well here is potentially problematic um when you have a um large group of large language models judges instead of a verifier you have then a stochastic verifier and that is able to give you some signal Um we we like the formal verification path um by by a lot. We think it's underinvested and it's also going to be more simple efficient. And you mentioned uh self-improving uh mathematical super intelligence. So what does self-improvement look like on a day-to-day basis when you're building this? Is it is it is it practicing on harder problems? Is it solving more number of problems or is it like learning new techniques automatically? Like what does that look like objectively? for sure. Um, if you think about a system that can reflect on its own mistakes and also the the the correct answers um and then build these knowledge back into the system um the skills library and then apply that the next time. That is how humans learn. For example, like I would think about the mistakes I have made in my past life and reflect on them and then try to do better next time. So for the AI mathematician system we are developing um we are having the model um reflect on its mistakes and then also reflect on what has worked successfully and just like I point everyone to a really good paper called Lego prover. Um it is the idea where you have an evolving skills library and then you want to have all the things that you um you know sort of new skills and um new knowledge that you um obtained from each attempt back into your skills library and then be applied the next time. Um our system have interesting uh mechanisms designed to do that relating to long-term um and short-term memory. uh and without kind of going too much into the technical nittyritty details um we by by having the sort of like skills learning design it is self-improving and also quite interestingly if you have another model in the same system that conjectures that is able to propose um mass problems quite well um then the conjecturer and the prover forming one self-improving loop um the conjecturer will set harder and harder curriculums based on the prover's performance and then rise like raise the bar for the proer to do better next time. And also the conjecture could set curriculums that attack um the the vulnerabilities of of the pro system. this sort of adversarial uh loop is quite interesting um to to build itself including AI and to solve the the data shortage problem and when it comes to lean just lean code available um you know one possible and and you tell me if it's reasonable or not people are using synthetic data they just do uh they generate data in different domains so in this domain if you just have a code gen model generate I don't know a billion lines of lean code by solving many many different problems. Can that serve as additional data or is it is it is not structurally adding anything new to the system because it's the system that generated the code in the first place. So how do you how do you think about synthetic data? For sure. For sure. At Axon we take very bold uh synthetic data. we um sort of invest heavily into having a large internet scale data set and we do need synthetic data best to work to to scale um because there's only so much that human experts can provide you. So now for example the running market quote for each IMO problem could be um hundreds of bucks each problem uh and and and that's just very hard to hard to scale. Um I think that for synthetic data to work um we need a verifier. It needs to be a verifiable domain. Otherwise it's a bit hard to if you think about a large language model generate like synthetically all the human dialogue data. It's very hard to like know if they are like usable or it's just going to be trash when it comes to training. Um in for example coding though if you have a program that compiles that is quite valuable. Um however you have certain challenges for example the signal could be sparse that means if I try to when the model has not seen much ling yet uh when it tries to generate uh synthetic link programs it might not compile like like just it just might not uh when especially when the proof gets really long. So to have like executable link code that that's that's a challenge in itself. Um we believe in kind of two ways of getting synthetic data. One is through auto formalization. Um there exists really amazing mathematical literature of the past like hundreds of years um on journals, textbooks, archive papers etc. If we can formalize that synthetically into into link code and then then we know that first of all all these mathematics are correct right. So you have these um most of them are correct unless you know the referee doesn't catch some bug in the paper. um you can you can then have interesting quite interesting and high quality mathematical content in link. So that's auto formalization and once you kind of plant the seed data by auto formalization at every difficulty level um we at Axium believe that you can synthetically expand that to much larger data set um 100 times 1,00 times it and when it comes to measuring the the performance and progress of the system like what public benchmarks or challenges like do you care about like is is there I'm sure there is something but What do you care about? For sure. There are many benchmarks in the formal mathematics community. Um very ancient one is mini F2F. Now mini F2F is basically completely solved. I believe that there is at least one model if not two that get 100% is saturated. Um and part of the reason why it could be saturated is maybe some folks have you know data contaminated like maybe have trained on it. Uh it has been like around for many many years. And there are other newer benchmark called pandm bench for example a benchmark collection of many years pandm exam. So that's pandm bench and there are other interesting benchmarks that are supposed to test the models mathematical capabilities. So combi bench um is testing the cominatorial ability of models and comtorials problems are generally quite hard. If you look at the one unsolved problem of this past year's IMO, it is a cominator problem. And you look at the two problems that were unsolved last year, the IMO silver metal performance um by Google Deepline, all I think both of them are also cominatorics problems. So somehow AI models just like not that great at combinotaurics and combi bench intends to test that 100 problems are combinatorics um quite hard and high quality and denovo. There's also this newer benchmark called mini CTX um v2 or the people using the version two of that benchmark. It's for long context reasoning testing for the um ability requires of a research AI mathematician that you need to be um multi-step and and kind of long context and requires um sort of beyond the Olympia math solving um abilities. We believe these benchmarks are great. Uh we like to test our models on holistic uh many many benchmarks to see where it's doing well, where it's not. Um one benchmark is a bit sort of thin as an evaluation uh metric. So we like to um put the model to test on on a lot of benchmark and we will also in the future um roll out um new benchmarks for the community as well. You made a an interesting observation that combinotaurics has been hard for some reason. Yeah. Any why is that? If you look at all the different fields, number theory, the topology, field theory, combinatorics, there's no reason specifically one has to be harder than the other, especially when it comes to an AI system, but yeah, it is. So, what's the how do you explain this? Yeah, 100%. Um, I have some uneducated guesses um as a mathematician. Um while in other domains such as algebra you look at the logical reasoning chain of thought in a proof um the the gap between one one step to the next is quite small um and perhaps the similar pattern of deduction has happened in other similar problems as well. So um whenever you have a dfentin equation um of say like you know um a x to the n plus b y to the n equals cz to the n um you will try to and then xyz are required to be integers you will assume xyz are um the common greatest common divisor is one so so that that kind of standard tricks really um exist in a lot of problems in commons however there's more creativity required um the leap from one step to another is quite large. That's why what people call them aha moments. It's like aha. Um I I don't actually know how I could think of that but somehow I did. Um a lot of the commentator problems especially at the IMO and pandm level require that sort of urea moment and creative thinking. So that's a little bit harder to pack the match. Um and and each problem is quite designed to be quite different. And that's that's one reason. Another reason I would say is cominator's problem involve a bit more multimodality um part of mathematics that's quite interesting because if you look at math and um a lot of problems involve symbolic expressions. Now symbolic expressions are based on text. Um you have formula equation. Um for common targets problems you need um sometimes graphics. Um a lot of pro common targets problems are set up as a chessboard. You have a grid. Now okay this grid is a little bit hard to represent. You will probably express the grid on a on a computer different way than literally drawing a grid. But then the visual and spatial reasoning components are are lost in that translation. graphs for example graphs theory really big part of comnotaurics and um connected to many other branches of comnotaurics uh you look at the graph and then you try to add a line or you you know erase a line you perturb the graph all these structure are a bit lost um that's why also geometry requires a whole different engine to solve so for all the Olympia mass contest generally alpha geometry is different from the alpha proof system um and in this past year's IMO there are other teams using um different geometry engine to to solve the uklean geometry similarly because of the visual component um of the problem. Interesting. So it's the whenever you need multimodal like combining different ideas from different subdomains to build a solution it becomes a little bit harder versus like uh if you can solve in a in a self-contained subdomain where it can just go from start to end in one go. Yeah. I think generally if you see like the problem having C figure one and then there's a figure drawn you'll be like oh like it might be right um interesting and so on on the flip side then something like algebra would be would be like AI would would crush something like that what so if you had to pick know a couple of domains where AI has absolutely crushed it within mathematics like what what are those AI has absolutely crushed I I don't think AI has absolutely crushed crushed any domain yet. Quite interestingly, I think AI I think AI has crushed the domains it has seen. So specifically if you AI a mass problem that has seen in its training is that likely it's going to do well. Interesting. We see large language models repeatedly um doing retrieval um looking up from the textbooks um what are you know arguments that could be applied here. Um sometimes it's actually not the problem but sometimes it gets it. For example, I think recent there are erdos problems um that were previously marked open um but then are actually not open because it has already existed in some German mathematicians you know sketch paper for example. Um we at FCM want to create AI mathematicians that can solve um problems it has not seen um using formal and formal reasoning and we think that is an incredibly ambitious goal. But having an AI system that can consistently and correctly solve all branches of mathematics like algebra analysis, number theory, um and especially real research mathematics beyond um IMO benchmarks that are competition for very bright kids. Um we think that's really interesting. Amazing. Now let's let's move to infra and compute because what you're what you're you're taking on a massive problem and um the infra footprint I can imagine is as you do more and more it's going to be big. So as a starting point can you talk about your compute footprint today as you're building the the the first big version of this? Yeah. Um so we believe that um to do this project well we need scaling. uh we also need great simple efficiency. So I think it's like as peril famously put it is really multiplying these two two axis um we believe that this is going to be an effort that needs a lot of compute. We are currently um using our compute very smartly and then we expect to scale as it goes. Right. And are there parts of the stack that you had to build yourself because nothing worked off the shelf or how do you think about roughly what did you what can you use off the shelf versus stuff that you have to build yourself? Yeah. Uh while we try our best to reduce the amount of infra we build in-house we actually build a lot of infra in house. So uh we we for example spend a lot of um you know engineering cycles on lin tooling. Um there are a lot of tools from the open source community but while those are interesting inspirations as to what tools are needed they might not be sort of industry strength uh quality. So instead of for example fixing potential bugs in these tools we will we will build all these lin tooling ourselves in house. Um that's one big part of INFA. Um and there are other parts for example just reducing a lot of the American concrete research ideas into an engineering problem. Um we we would love to have generalist engineers join us to build those out. Um for the for the compute part we you know we we kind of take a similar approach from a lot of other other startups right when we are really small scale we use uh on on demand uh you know like buy as you go pay as you go uh compute. Um and then when we scale up we will probably have um more kind of fixedterm contract um with um compute providers that can also help us with a lot of the necessary infra but also handle some of the um infra related to training ourselves and as you productize the this what you're building AI mathematician where are the the initial use cases that you're envisioning is it research labs finance engineering some of that else. Yeah. So, I like to take tours at the computer history museum just on some of the some of the days I enjoy just wandering around. In the history of Silicon Valley, there hasn't really been a case where there is a step function change in technology but there is an absence of business model. Um this is something that um yeah that we strongly believe in and we also believe that there has been you know obviously like bad execution of a sound business model that then you know make the entire thing not able to work out but we believe in lot of applications being unlocked because of the fundamental uh phase transition of the technology. Um and some very initial ideas are formal verification of hardware, formal verification of software. Um so that is very much tied to the formal math system we're are building because if you are like an informal math model um you can be really strong in math but it cannot do formal verification well in these um safety critical applications. Um protocol verification and cryptography is also very interesting. Um and when you can do math consistently correctly, you can also unlock for example finance engineering um requiring like for example complex differential equations um optimization and and many more. Can you quickly explain formal versus informal verification? Yeah. Uh so informal side is not um quite verified. Um it is you know the stochastic verifier of um large large language models and they will have you know large language models serve as judges. Um the formal verification side is um rei you know one one key part is the ser the prover is likely that we're relying on right and when you when you look to uh the team you're building you talked about what kind of people are joining or are about to join. So how do you think about constructing a team here? Uh just maybe double click on is it know pure mathematics systems formal methods intrag engineer you need a bunch of superstars to join here. So maybe talk about what do you need uh to to ship this. Yeah. So we believe in three important pillars um AI programming languages and math. Uh we believe in having folks with decades of experience in each of these pillars to come together. Um on the AI side, you need amazing um applied AI engineers and researchers. Um on the programming language side, we really have I think one of the best teams in the world applying machine learning to programming language and this like AI systems flavor of folks. Um and then on the math side, we want mathematicians who are also hands-on engineers um to work together with with the team to have these mathematic u like mathematicians ideas put them into into engineering deployment and let's say you know you build this you ship this and you know it's it's starting to work now once the AI mathematician system is live can you talk about throwing more compute or hardware, how does it make a difference? How how quickly will it make a difference? Because if you give me a system and I show up with a massive GPU cluster, will I be able to do a lot with it? So, can you talk about how the system can scale with with comput and infra? Yeah, we believe scaling law also works in this domain. Um, we don't necessarily see any fundamental issue why it shouldn't. Um there are for example difficulties and bottlenecks on the infra side especially when it comes to lean but those are not sort of insurmountable challenge we can I think we fully expect scaling lot to work pretty well here and in fact perhaps um with greater impact because it's really um the sample efficiency is going to be better and if if Axiom fully works according to what you have at your vision And what what would be the biggest delta in the world? Like what would become cheaper or faster or better in the world in a way that everyone would notice? Like what are you what are you expecting to see in the world? Yeah, I think I would love folks to imagine AI goals at your finger fingertip. So deploying many AI mathematicians into all the complex systems that are currently not theoretically understood. um these systems are quite complex to make sense theoretically but then having an AI mathematician working together alongside with the applied scientists um we think that's going to be a gamecher and specifically when the price of reasoning becomes more elastic there is more demand um Javon's paradox is our opportunity we expect to see new markets use cases being unlocked because of the surge in supply of outlier mathematical reasoning capabilities. And so for example, if you can think of a a hedge fund um now being able to afford the best and and ncout the best AI mathematician as quantum traders um in in markets that previously were not on lock because say the total trading volume just too low for the uh unit economics makes sense because you know employing a really smart human quad that costs a lot of money each year. like what would that hedge fund do and what would that market be like? Um that's a question of mass intelligence. If you think about say a a cloud compute provider now having um AI um engineers to work on the routine IT optimization task, right? And and these AI engineers are of the caliber of say meta um you know the strongest meta AI engineers um that are currently being paid say like one or two millions a year, right? um how would that change that industry? I think those are the questions that get us extremely excited about and we believe that you cannot get there if you don't make progress on the mathematical capability of AI. Amazing. With that, uh we're at the rapid fire round. I'll ask a series of questions and would love to hear your answers in 15 seconds or less. You ready? Yeah. All right. Question number one. What's your favorite book? Um proofs on the book. Uh there's idea by Erdoger is a book of all the most delicate mathematical proofs and I enjoy reading it. Amazing. Which historical figure do you admire the most and why? Alan Turing. I think he really bridges um theory and applications and especially in the time where very few people believed that was possible. Also a very high stake situation. Yeah. Yeah. Yeah. Um what's the one thing about mathematical AI that most people don't get? Yeah. It is the only domain where you can do conjecturing and do um you know conjecture proof and discover um with consistent verifiable signals like without sort of messy real world data right unlike say self-driving or or robotics you need to actually go out there to to get those data and discover mass is the perfect sandbox for um self-improving AI right what separates great AI products from the merely good ones I think the great ones actually compound things and unify things. So, you can do a lot in it. It should be a frictionless, delightful experience. Um, and I think it also should teach the user something that they haven't quite thought about, but then just feels like magic when you try. I'm like very of the Steve Jobs camp, right? What have you changed your mind on recently? Um, I was pretty averse to caffeine before. I think that was like a drop. Uh now I have embraced it. Um probably thanks to Founding Action. Yes. Welcome to the world of caffeine. It's going to be a lifelong friendship. Um what's your wildest AI prediction for the next 12 months? Next 12 months, I think we are going to see an AI mathematician solve a really hard problem. I don't want to say necessarily millennium prize problem but I think that um AI form metal discovery is going to shock the world right all right final question what's your number one advice to founders who are starting out today I think you really need to build something extremely hard like pick the hardest problem you can think of I know that with AI a lot of things could feel like great opportunities um unlocking things that haven't been done before um quite easily and conveniently. But I think to build a generational lasting business, you really need to pick on something that is very hard because hard problems attract talent like a talent magnet. People want to work with the best of the best on the hardest most stubborn problems. So I would say build something that is very hard and I think um Lisa uh Sue from uh AMD also gave a similar advice that I took to heart. Build something very hard. Amazing. Um, Karina, this has been a brilliant, brilliant discussion. Love the the insights, what you're building. Just the the sheer ambition of what you're building is exciting. So, thank you so much for coming on to the show. Thank you so much for having me. All right. Get
